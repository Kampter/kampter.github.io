[{"categories":["技术实现"],"date":"April 18, 2023","permalink":"https://kampter.github.io/blog/%E8%BF%91%E6%9C%9F%E5%81%9A%E7%9A%84%E5%87%A0%E4%B8%AAunity%E5%92%8Cue%E7%89%B9%E6%95%88/","section":"blog","summary":"风格化毒液投射物 请原谅我制作了一个毒液的技能特效。我原本想要制作毒液效果的，但是尝试了多次后一直不满意，就把部分素材拼凑成了毒液的技能特效，希望可以得到谅解。这里我用UE5来制作这个技能特效。\nTexture的思路：用cross section + histogram scan制造一条线，通过不断叠加noise制造细节，并且用blend的subtract模式减掉球状的shape，用来模拟液体的感觉。最后用Directional Warp制造速度的感觉。\n原本Trail是不需要有mesh的，但是我希望力大出奇迹（bushi），只是希望最终呈现的效果更加立体吧。这里用 point jitter制造noise并且把球体copy to point 来制造这个不规则的球状物体作为 projectile 的mesh。\nMaterial的部分我分成三块，第一块是 Projectile，我希望用Fresnel来描述mesh的边缘发光的感觉。同时在这个Material里面我还添加erosion的效果在alpha mask中，希望在Niagara中可以驱动这个erosion 制造飞溅的小液滴的效果，而不是用particle来实现。除此之外我还用一张noise 贴图来驱动vertex posion，希望制造液体飞出主体但是与主体牵连的感觉(液滴表面张力)。 Smoke的部分，用一张Noise作为UV的distortion添加在Main Noise Texture上面，并且添加圆形的mask和depth fade减弱边缘。Trail部分与Smoke基本相同，在Mask部分我用了横向的Gradient Mask，希望Trail的尾部逐渐虚化掉。\nNiagara实现：发射三个projectile mesh，用shape location + initial mesh orientation + rotate around point 实现绕圈旋转的效果，同时发射location event作为后续粒子的source，额外添加一个Light renderer实现ambient light的效果。两层的splash用projectile material的erosion动画实现，额外添加一些速度和force来制造细节。Trail的部分也是两层，一层较宽较浅色，整体细节比较少，一层颜色鲜明，变化的noise和细节增加。\n抖动上升的会溶解的气泡 https://www.youtube.com/watch?v=bN84YxaBEGw 透明泡泡的材质有参考大佬的教学，主要是想要知道泡泡的 Iridescent shade究竟是怎么实现的, 毕竟在测试的这么短时间里手写扩展URP的Lit shader还是挺有挑战的。大佬的shader中用NdotV作为diffuse来采样gradient，这种手法与之前做过的卡通渲染有异曲同工之妙。\n颜色的部分我用比较讨巧的方式，在Substance designer中 sample 一个ground truth的gradient map，并且把颜色承载Fresnel里面控制边缘颜色\n气泡中还有那种波纹流动的液体感觉，Ben Cloward 的做法是直接采样一张类似的noise 贴图来模拟，我这里用了屏幕空间叠加的方式。用一个随时间变化的Twirl voronoi noise来模拟，并且叠加在Scene Color上面（需要在URP开启Opaque texture）\n消散的思路：一样通过输入noise，在noise上偏移一个很小的值并且输入smoothstep再相减来获取边缘的Mask。我额外计算一层Mask + edge后的结果输出到Alpha中，这样既可以有发光mask，又可以有alpha mask。\n在VFX graph制作比较简单，在一个区域内随即发射添加刚刚制作的shader的小球，让小球随着时间不断上升，并且让小球的scale_y随着时间不断变化，添加一些动态效果。\n然后根据lifetime控制消散动画，并且在合适的时间上设置GPU event添加第二套粒子作为小球的消散后的效果。\n这里有个小遗憾是没能实现第二套粒子精确发射在小球的消散边缘。查阅资料后发现了UE的实现办法：\nhttps://realtimevfx.com/t/spawning-particles-from-the-edge-of-dissolving/18715\n也就是额外在Niagara的module script中额外计算一套消散mask，在计算出的边缘mask位置做判断发射粒子。VFX graph可能可以但是我现在比较菜还没有实现。\n另一个想到的比较通用的方式是在Houdini中提前Bakex消散动画的一张VAT，然后在小球需要消散的时间节点上播放这个动画，但是由于时间有限，以后我会尝试这个方案，参考如下。\nhttps://docs.google.com/presentation/d/1n6ykDR_O_57FLuRqrO0Dlb9U0GYNZ-QLPX8DVg7asqE/edit#slide=id.p10\n写实风格的雷击命中特效 闪电的部分会大致拆分成三部分来实现。第一部分是从天空中垂直落下的雷击，以蓝紫色为主体，伴有一些亮度较高的红黄色部分。第二部分是地面的命中痕迹，包括蓝紫色的感电效果，红黄色高亮的燃烧效果，以及黑灰色的烧焦痕迹。第三部分是地面由命中位置飞溅出来的碎块。\nMain Strike部分：\n贴图的部分，我使用用四层不同Scale的perlin noise增加细小线条的细节。做法是cross section后用historgram scan控制线条的宽度，并且每一层用cells1这个边缘很锐利的noise\n来增加每一个线条的细小边缘。三层的细节加上一层的主体基本上就有我比较想要的效果（反复在引擎内测试）。另外再叠加1-2次的Blur来制造闪电线条周围发光的光晕感觉。\nShader的部分我是用Shader graph来完成的。整体思路就是用Tilling and Offset来控制Strike 贴图随时间偏移，为了增加更多细节，在UV上面额外添加一层可以控制Tilling 和 Speed 的Noise 贴图。Erosion的部分直接把贴图当作alpha透明来使用，减去随时间变化的Noise贴图。最后额外添加一个IsTopStart来制造闪电的下落的动态下效果，思路也比较简单，用UV的V方向也就是垂直方向来添加一个偏移值，闪电两侧会被虚化掉。\nMesh制作我使用Houdini，尽管单面片的开销更低，我还是希望使用插片mesh，获得更好的立体感，思路也很简单。创建一条线，用point jitter来增加变化，用sweep来获得面片并且旋转90度生成插片，最后调整刚刚Sweep过的UV以及设置50 uniform scale，为了导入引擎更方便。\n在VFX graph中是这样使用的，一共四层闪电效果。每一层效果原理几乎相同，只对参数有细微调整。在Particle初始化中，我会设置好lifetime, 我会希望前两个主体的闪电是立即出现，而后会有2-3根细小的闪电再出现（希望有一点残影或者能量延迟的感觉）。同时闪电会有细微的位置和旋转角度变化。\n主体的部分我会使用 Age over lifetime + sample curve 来设置刚刚shader中暴露好的参数。比如在主体色的部分，我会设置蓝紫色的主体以及少量的红黄色高能量的闪电。我还会设置闪电的大小，希望闪电的在X和Z方向的scale变化是比较硬的变化，来制造一种迅猛变化的感觉。\nSpark的部分：\n贴图我比较偷懒继续使用刚刚Stike的贴图，VFX graph中的设置也比较简单，设置好随时间变化的大小，旋转等参数即可。\nGround Crack 的部分：\n贴图的思路，用Cells4 + shape mapper制造地裂的原型基础感觉, 用shape 和perlin noise接入 warp来制造细节叠加进来。对于需要删除的部分用Blend 的subtract 模式不断尝试剔除不需要的地方。当效果基本到自己想要的感觉后，再用不同的blur增加地面裂开的宽度，并且模拟地面被劈开后缝隙中的感电和火光带来的辉光效果。\nShader的部分与Stike的思路其实差不多。区别在于Erosion的边缘我希望有一些渐变而不是很硬的过度，因此用smoothstep来代替subtract。这里我几乎对每一个参数，比如color, alpha, emission 都有分别添加erosion，希望分别控制地面的效果，同时希望感电的区域出现在地裂的里面。对于地面裂开的地方，我使用了POM视察映射，希望增加更多细节。\nVFX graph中，我是用三层不同的Crack效果，第一层是蓝紫色的感电效果，第二层是红黄色的高能量燃烧效果，最后是几乎纯黑色的焦痕，并且焦痕会持续更久一些直到完全消散掉。\n","tags":["Substance Designer","Houdini","Unreal Engine","Niagara System","Unity","VFX graph"],"title":"近期做的几个Unity和UE特效"},{"categories":["学习笔记"],"date":"April 5, 2023","permalink":"https://kampter.github.io/blog/urp%E5%B1%8F%E5%B9%95%E7%A9%BA%E9%97%B4sobel%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/","section":"blog","summary":"什么是边缘检测 根据Wiki说明，边缘检测是一种用于识别图像变化的明显的区域的技术。因此这种基础技术被广泛应用与图像处理和计算机视觉中用以查找出物体的边缘。边缘检测算法的目的是发掘图像中不连续火图像亮度急剧变化的点。而常见的几种算法用于便边缘检测技术包括Sobel， Prewitt, Roberts 和 Canny。其中Canny 边缘检测器是罪被广泛应用的算法，它使用多阶算法结合，于1986年被John F. Canny最先开发出来并且随后广泛使用。\n为什么边缘检测重要 边缘检测是图像处理的重要组成部分，因为它有助于计算识别和将图像分离成不同的部分 。 它用于图像处理、计算机视觉和机器视觉等领域的图像分割和数据提取。 检测图像亮度的急剧变化的目的是捕捉重要事件和属性的变化。 边缘是与图像相关的最重要的特征之一。 我们通过图像的边缘了解图像的底层结构。 因此，计算机视觉处理管道广泛使用边缘检测。\nSobel 边缘检测 简单地说，我们对每个片段的四个边邻居进行采样，并对采样属性的差异求和。 小的差异表明表面是一致的，我们很可能不在边缘上，而大的差异表明采样属性的突然变化，我们很可能在边缘像素上。\n我们将用来计算 Sobel 差分的函数可以表示为：\ns为纹理样本，是原像素点的位置 中、上、下、左、右 multiplier 是一个标量输入控制参数 线性影响值 bias 是一个标量输入控制参数，用于减少最终结果中的噪声伪影 URP代码实现 URP中需要在custompass中开启Scene normal的支持，方法参考官方实现的SSAO效果。\nConfigureInput(ScriptableRenderPassInput.Normal); 在Shader中便可以对CameraNormalsTexture采样\n#include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/DeclareNormalsTexture.hlsl\u0026#34; float4 SampleNormal(float2 uv) { return SAMPLE_TEXTURE2D(_CameraNormalsTexture, sampler_CameraNormalsTexture, uv); } 对法线和深度采样的方式几乎完全相同，采样法线和深度两种方式\nfloat4 SobelSampleNormal(float2 uv, float3 offset) { float4 pixelCenter = SampleNormal(uv); float4 pixelLeft = SampleNormal(uv - offset.xz); float4 pixelRight = SampleNormal(uv + offset.xz); float4 pixelUp = SampleNormal(uv + offset.zy); float4 pixelDown = SampleNormal(uv - offset.zy); return abs(pixelLeft - pixelCenter) + abs(pixelRight - pixelCenter) + abs(pixelUp - pixelCenter) + abs(pixelDown - pixelCenter); } 只使用邻近的四个的原因是为了减少不必要的边缘检测\n完成 SobelDepth 和SobelNormal的计算后，将两种采样结果合并\nfloat sobelOutline = saturate(max(sobelDepth, sobelNormal)); 此时还需要处理一些缺陷\n物体与物体连接处没有边缘 视线与物体夹角较小，接近平视时因为像素的问题会导致一部分物体表面都会出现描边颜色而不是很细的线。 Reference\nSobel Outline with Unity Post-Processing · Vertex Fragment URP Sobel Edge Detection (github.com) https://www.youtube.com/watch?v=LMqio9NsqmM\u0026amp;t=4s ","tags":["Unity","HLSL","C#","Render Feature"],"title":"URP屏幕空间Sobel边缘检测"},{"categories":["学习笔记"],"date":"April 5, 2023","permalink":"https://kampter.github.io/blog/blender%E6%8F%92%E4%BB%B6%E5%88%B6%E4%BD%9C%E5%BF%83%E5%BE%97/","section":"blog","summary":"前言 一个偶然的机会接触到 Blender插件编写，一直以来我都是(曾经是) Maya以及Houdini软件的拥护者，但是Blender 的开源社区逐渐壮大也不能不引起重视，那么是时候了解一下Blender的相关内容。\nBlender的交互层都是用Python来编写的，这就相当友好了。作为一个使用过python用来数据分析以及编写爬虫爬 *** 内容的人表示这很开心。\n基础模板 bl_info = { \u0026#34;name\u0026#34;: \u0026#34;HelloAddon\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;作者\u0026#34;, \u0026#34;version\u0026#34;: (1, 0), \u0026#34;blender\u0026#34;: (2, 79, 0), \u0026#34;location\u0026#34;: \u0026#34;View3D \u0026gt; Tool Shelf \u0026gt;HelloAddon Panel\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;插件描述\u0026#34;, \u0026#34;warning\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;wiki_url\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;3D View\u0026#34;, } import bpy import logging from . import model_data from . import command_operators from . import view_panels def register(): bpy.utils.register_module(__name__) command_operators.register() view_panels.register() def unregister(): bpy.utils.unregister_module(__name__) command_operators.unregister() view_panels.unregister() if __name__ == \u0026#34;__main__\u0026#34;: register() 整体思路还是比价简单的。以上代码就是套路，不需要理解，主体为bpy.utils.register_module(name)，作用是注册import进来的所有模块。至于command_operators.register() 与 view_panels.register()则代表其他非模块相关的注册。这里的代码以后要新建工程直接ctrl c即可。直接运行main函数，就可以注册插件。z注册插件后就可以编写自己的插件内容并且分别在 register() 和 unregister() 中调用。\n编写一个 check object命名的功能吧 我希望我的这个插件可以实现的功能是遍历当前场景内全部的object，然后根据命名规则判断正确还是错误，命名规则如下。\n​\t\u0026lt;asset_type\u0026gt;\u0026lt;descriptive_name\u0026gt;\u0026lt;descriptive_name\u0026gt;_\u0026lt;##\u0026gt;\n其中\u0026lt;asset_type\u0026gt; 可以为 char, prop, veg, veh, arch 和 tree 中间的\u0026lt;descriptive_name\u0026gt;可以有2-3个不同的名字，并且全部为小写的英文字母 最后的\u0026lt;##\u0026gt; 是数量，从01 最大可以到99 并且把最终的结果输出到一个html网页中并渲染出来。最终效果如下\n先写基础模板把我的CheckNamesOperator 注册上去\nimport bpy def register(): bpy.utils.register_class(CheckNamesOperator) def unregister(): bpy.utils.unregister_class(CheckNamesOperator) if __name__ == \u0026#34;__main__\u0026#34;: register() # test call bpy.ops.object.check_naming() 功能实现部分的思路如下。先遍历场景内的object， 然后读到这个object的当前命名内容，把他当作一串string。整体思路有点类似之前编程课写的string parser。这里把命名string用split(\u0026quot;_\u0026quot;)拆分为三部分并且放在一个tags list中\nPrefix 第一部分的asset_type 是prefix 用tags[0] 提取，判断第一部分的类型，可以预先用一个list把valid prefixes提前写好，判定提取的string是否在list中\n# Prefix if tags[0] in valid_prefixes: prefix_ok = True Suffix 第三部分的## 用tags[-1]提取, 最后一部分的内容是长度为2 的数字。数字的判定用python自带的ValueError来判定，如果不报错就是数字。\n# Suffix nan = False padding = len(tags[-1]) == suffix_padding try: num = int(tags[-1]) except ValueError: nan = True Identifiers 中间的部分descriptive_name用tags[1:-1] 提取，先判断中间部分有几个descriptive_name, 规则只允许2-3个。其次要求descriptive_name全部为小写的英文字母\n# Identifiers errors = 0 if len(tags) - 2 not in num_indentifiers_required: errors += 1 for id in tags[1:-1]: if not id.isalpha(): errors += 1 if not id == id.lower(): errors += 1 if errors == 0: ids_ok = True 这样最核心的判定就完全结束了。现在来编写输出的部分\nclass CheckNamesOperator(bpy.types.Operator): \u0026ldquo;\u0026ldquo;\u0026ldquo;Check Object Names\u0026rdquo;\u0026rdquo;\u0026rdquo; bl_idname = \u0026ldquo;object.check_naming\u0026rdquo; bl_label = \u0026ldquo;Check Object Names\u0026rdquo;\n@classmethod def poll(cls, context): return context.active_object is not None def execute(self, context): scene_objects = [[obj[0], obj[1]] for obj in bpy.data.objects.items()] results = [] errors = 0 for obj in scene_objects: v = validate_name(obj[0]) if not all(v): errors += 1 results.append({ \u0026#39;object\u0026#39;: obj[0], \u0026#39;prefix\u0026#39;: \u0026#39;OK\u0026#39; if v[0] else \u0026#39;FAIL\u0026#39;, \u0026#39;ids\u0026#39;: \u0026#39;OK\u0026#39; if v[1] else \u0026#39;FAIL\u0026#39;, \u0026#39;suffix\u0026#39;: \u0026#39;OK\u0026#39; if v[2] else \u0026#39;FAIL\u0026#39;, \u0026#39;pass\u0026#39;: all(v) }) if errors: self.message_errors(results) else: self.message_ok(message=\u0026#39;No problems found!\u0026#39;) return {\u0026#39;FINISHED\u0026#39;} def message_ok(self, message=\u0026#34;\u0026#34;, title=\u0026#34;OK!\u0026#34;, icon=\u0026#39;INFO\u0026#39;): def draw(self, context): self.layout.label(text=message) bpy.context.window_manager.popup_menu(draw, title=title, icon=icon) def message_errors(self, results, title=\u0026#34;Errors Found!\u0026#34;, icon=\u0026#39;ERROR\u0026#39;): scene_file = bpy.path.basename(bpy.data.filepath) scene_folder = path.dirname(bpy.data.filepath) base_file_name = \u0026#39;.\u0026#39;.join(scene_file.split(\u0026#39;.\u0026#39;)[0:-1]) + \u0026#39;.html\u0026#39; out_html_file = path.join(scene_folder, base_file_name) with open(out_html_file, \u0026#39;w\u0026#39;) as f: f.write(\u0026#39;\u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026#39;) # Styles f.write(\u0026#34;\u0026#34;\u0026#34; \u0026lt;style type=\u0026#34;text/css\u0026#34;\u0026gt; table { margin: 0 auto; text-align: left; } tr.error{ background-color: #990d0d; color: #ffd5d5; } tr.ok { background-color: green; color: #1a2615; } table { margin: 10% auto; width: 40%; background-color: dimgray; } th { color: #b9b9b9; } body { background-color: rgb(26, 26, 26); } \u0026lt;/style\u0026gt;\u0026lt;/head\u0026gt;\u0026lt;body\u0026gt;\u0026lt;table\u0026gt; \u0026#34;\u0026#34;\u0026#34;) # Header f.write(\u0026#39;\u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Object\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;Prefix\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;Ids\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;Suffix\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt;\u0026#39;) # Results for r in results: row_class = \u0026#39;ok\u0026#39; if r[\u0026#39;pass\u0026#39;] else \u0026#39;error\u0026#39; f.write( f\u0026#34;\u0026lt;tr class=\\\u0026#34;{row_class}\\\u0026#34;\u0026gt;\u0026lt;td\u0026gt;{r[\u0026#39;object\u0026#39;]}\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;{r[\u0026#39;prefix\u0026#39;]}\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;{r[\u0026#39;ids\u0026#39;]}\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;{r[\u0026#39;suffix\u0026#39;]}\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt;\u0026#34;) f.write(\u0026#39;\u0026lt;/table\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#39;) # Open Generated html Results webbrowser.open(out_html_file) 这是一段我借鉴来的代码。主要涉及到几部分内容。\n这是一个class 的 CheckNamesOperator，也就是我们的检查器对象。 其中最核心的部分在 exeute() 内，用以运行刚刚的检查逻辑函数，并且把结果保存在results[] 这个list中 输出王爷的部分，其实这是一个调用IO的函数，主要是遍历刚刚我们保存下来的results 数组并且逐行按照我们的目标效果编译出来的，中间还涉及到html 和css代码（虽然挺丑的）。最终把Html保存在Blender相同的目录下面。 检查代码效果 创建几个object，并且根据我们的目标规则，更改命名。可以看到场景内对对错错的object有好几个，虽然我不是专业的测试，但是基础的边界测试还是要有的。\n检查完自动弹出的检查结果窗口，很丑，但是可以提示我们哪里有错误，可以及时修改。\n再写一个Pie Menu的Object生成菜单 插件的逻辑 View3D_MT_PIE_template(menu) 用以继承官方的pie menu并且在下面添加我们自己的功能按钮，用pie.operator() View3D_OT_PIE_template_call(menu) 用以在场景内呼出这个菜单 每一个功能按钮单独用一个class实现，bl_idname就是在pie.operator()中调用的id, 每一个功能按钮都有一个execute用以运行。其中的功能可以自行设计，我这里就调用比如生成plane, cube, sphere的代码 额外编写一个add_hotkey()用以实现快捷键呼出菜单的功能，核心是这一段，用以调用场景内呼出菜单的效果。 km.keymap_items.new( VIEW3D_OT_PIE_template_call.bl_idname, \u0026#39;D\u0026#39;, \u0026#39;PRESS\u0026#39;, ctrl=True, shift=False) 最后的最后，把全部实现的功能都注册在register()和unregister()中就可以了。 插件在Blender中效果实现如下\n完整插件代码如下，可以直接复制并且安装在blender中，默认是shift + D呼出菜单。\nimport bpy from bpy.types import Menu \u0026#39;\u0026#39;\u0026#39; Reference List 1. https://www.youtube.com/watch?v=jfQTX293dw0 2. https://blender.stackexchange.com/questions/147894/is-there-a-way-to-add-a-custom-pie-menu-in-2-8 3. https://docs.blender.org/manual/en/latest/advanced/scripting/addon_tutorial.html \u0026#39;\u0026#39;\u0026#39; bl_info = { \u0026#34;name\u0026#34;: \u0026#34;Kampter Pie Menu\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Kampter\u0026#34;, \u0026#34;version\u0026#34;: (0, 0, 0, 1), \u0026#34;description\u0026#34;: \u0026#34;Pie Menus made by Kampter\u0026#34;, \u0026#34;blender\u0026#34;: (3, 40, 0), \u0026#34;category\u0026#34;: \u0026#34;Mesh\u0026#34; } addon_keymaps = [] def add_hotkey(): wm = bpy.context.window_manager kc = wm.keyconfigs.addon if not kc: print(\u0026#39;Keymap Error\u0026#39;) return # object Mode km = kc.keymaps.new(name=\u0026#39;Object Mode\u0026#39;, space_type=\u0026#39;EMPTY\u0026#39;) # here you can chose the keymapping. kmi = km.keymap_items.new( VIEW3D_OT_PIE_template_call.bl_idname, \u0026#39;D\u0026#39;, \u0026#39;PRESS\u0026#39;, ctrl=True, shift=False) addon_keymaps.append((km, kmi)) def remove_hotkey(): for km, kmi in addon_keymaps: km.keymap_items.remove(kmi) addon_keymaps.clear() class VIEW3D_MT_PIE_template(Menu): bl_label = \u0026#39;Kampter Mesh Generator Menu\u0026#39; def draw(self, context): layout = self.layout prefs = context.preferences inputs = prefs.inputs pie = layout.menu_pie() pie.operator(\u0026#39;kampter.plane\u0026#39;) pie.operator(\u0026#39;kampter.cube\u0026#39;) pie.operator(\u0026#39;kampter.sphere\u0026#39;) pie.operator(\u0026#39;kampter.cylinder\u0026#39;) class VIEW3D_OT_PIE_template_call(bpy.types.Operator): bl_idname = \u0026#39;kampter.call\u0026#39; bl_label = \u0026#39;Kampter Pie Menu\u0026#39; bl_description = \u0026#39;Calls pie menu\u0026#39; bl_options = {\u0026#39;REGISTER\u0026#39;, \u0026#39;UNDO\u0026#39;} def execute(self, context): bpy.ops.wm.call_menu_pie(name=\u0026#34;VIEW3D_MT_PIE_template\u0026#34;) return {\u0026#39;FINISHED\u0026#39;} class CREATE_PLANE(bpy.types.Operator): bl_idname = \u0026#39;kampter.plane\u0026#39; bl_label = \u0026#39;Plane\u0026#39; def execute(self, context): bpy.ops.mesh.primitive_plane_add(enter_editmode=False, align=\u0026#39;WORLD\u0026#39;, location=(-2.25724, -2.46012, 2.15026), scale=(1, 1, 1)) return {\u0026#39;FINISHED\u0026#39;} class CREATE_CUBE(bpy.types.Operator): bl_idname = \u0026#39;kampter.cube\u0026#39; bl_label = \u0026#39;Cube\u0026#39; def execute(self, context): bpy.ops.mesh.primitive_cube_add(enter_editmode=False, align=\u0026#39;WORLD\u0026#39;, location=(0, 0, 0), scale=(1, 1, 1)) return {\u0026#39;FINISHED\u0026#39;} class CREATE_UV_SPHERE(bpy.types.Operator): bl_idname = \u0026#39;kampter.sphere\u0026#39; bl_label = \u0026#39;UV Sphere\u0026#39; def execute(self, context): bpy.ops.mesh.primitive_uv_sphere_add(radius=1, enter_editmode=False, align=\u0026#39;WORLD\u0026#39;, location=(0, 0, 0), scale=(1, 1, 1)) return {\u0026#39;FINISHED\u0026#39;} class CREATE_CYLINDER(bpy.types.Operator): bl_idname = \u0026#39;kampter.cylinder\u0026#39; bl_label = \u0026#39;Cylinder\u0026#39; def execute(self, context): bpy.ops.mesh.primitive_cylinder_add(enter_editmode=False, align=\u0026#39;WORLD\u0026#39;, location=(-0.492159, -3.12005, -1.62732), scale=(1, 1, 1)) return {\u0026#39;FINISHED\u0026#39;} def register(): bpy.utils.register_class(VIEW3D_MT_PIE_template) bpy.utils.register_class(VIEW3D_OT_PIE_template_call) bpy.utils.register_class(CREATE_PLANE) bpy.utils.register_class(CREATE_CUBE) bpy.utils.register_class(CREATE_UV_SPHERE) bpy.utils.register_class(CREATE_CYLINDER) add_hotkey() def unregister(): bpy.utils.unregister_class(VIEW3D_MT_PIE_template) bpy.utils.unregister_class(VIEW3D_OT_PIE_template_call) bpy.utils.unregister_class(CREATE_PLANE) bpy.utils.unregister_class(CREATE_CUBE) bpy.utils.unregister_class(CREATE_UV_SPHERE) bpy.utils.unregister_class(CREATE_CYLINDER) remove_hotkey() if __name__ == \u0026#34;__main__\u0026#34;: register() Reference\nblender插件开发入门 - 知乎 (zhihu.com) https://www.youtube.com/watch?v=jfQTX293dw0 https://blender.stackexchange.com/questions/147894/is-there-a-way-to-add-a-custom-pie-menu-in-2-8 https://docs.blender.org/manual/en/latest/advanced/scripting/addon_tutorial.html ","tags":["blender","plugin"],"title":"blender插件制作心得"},{"categories":["学习笔记"],"date":"April 5, 2023","permalink":"https://kampter.github.io/blog/urp%E5%B1%8F%E5%B9%95%E7%A9%BA%E9%97%B4%E7%8E%AF%E5%A2%83%E5%85%89%E9%81%AE%E8%94%BD/","section":"blog","summary":"Screen Space Ambient Occlusion: Enhancing Realism in Video Games Introduction Video games have come a long way since their inception, and the quality of graphics has improved significantly. One aspect that helps in achieving realistic visuals in video games is Screen Space Ambient Occlusion (SSAO). SSAO is a technique that enhances the depth and realism of the game by simulating the way light behaves in real life. In this blog post, we will discuss what SSAO is, how it works, its benefits in video games, and how it can be implemented.\nWhat is Screen Space Ambient Occlusion? Screen Space Ambient Occlusion is a technique used in video games to simulate the way light interacts with objects in real life. In video games, SSAO is used to enhance the depth and realism of the game by simulating the shadows that are created when light is blocked by objects. SSAO calculates the amount of ambient light that reaches a pixel on the screen by taking into account the occlusion of that pixel by surrounding objects.\nHow Does Screen Space Ambient Occlusion Work? SSAO works by analyzing the depth and normal information stored in the game\u0026rsquo;s frame buffer. The technique calculates the amount of ambient light that should be reaching a pixel by analyzing the surrounding pixels. SSAO simulates the way light behaves in real life by taking into account the occlusion of a pixel by surrounding objects. By adding shadows to the game, SSAO enhances the depth and realism of the game\u0026rsquo;s graphics.\nBenefits of Screen Space Ambient Occlusion The use of SSAO in video games has several benefits. Firstly, it enhances the realism of the graphics by simulating the way light behaves in real life. This makes the game world feel more immersive, and the player feels more connected to the game. Secondly, SSAO adds depth to the game by simulating the way shadows are created when light is blocked by objects. This adds a sense of realism to the game world, making it feel more natural. Lastly, SSAO is a relatively inexpensive technique that can be used in real-time applications, making it a popular choice among developers.\nImplementing SSAO in Video Games Implementing SSAO in video games can be done using different techniques. Here are some of the most common methods used by game developers:\nScreen Space Ambient Occlusion Screen Space Ambient Occlusion is the most common method used by game developers. This technique uses the depth and normal information stored in the game\u0026rsquo;s frame buffer to calculate the amount of ambient light that should be reaching a pixel. SSAO is then applied to the pixel to simulate the way light behaves in real life.\nHorizon-Based Ambient Occlusion Horizon-Based Ambient Occlusion is a technique that takes into account the horizon of the game world. This technique calculates the amount of ambient light that should be reaching a pixel by analyzing the distance between the pixel and the horizon. This method is more computationally expensive than SSAO but provides more accurate results.\nMulti-Scale Ambient Occlusion Multi-Scale Ambient Occlusion is a technique that uses different scales to calculate the amount of ambient light that should be reaching a pixel. This method takes into account the distance between the pixel and the objects in the game world to provide more accurate results.\nWhen implementing SSAO, it is essential to consider the limitations of the technique. SSAO may not be effective in simulating the way light behaves in open spaces or in situations where the light source is very bright. It can also create artifacts in the game world, such as halos around objects. To overcome these limitations, game developers can use other techniques such as Horizon-Based Ambient Occlusion or Multi-Scale Ambient Occlusion.\nLimitations of SSAO While SSAO is a useful technique for enhancing the realism of video games, it does have its limitations. SSAO is only effective in simulating the way light behaves in real life in certain situations. For example, SSAO is not effective in simulating the way light behaves in open spaces or in situations where the light source is very bright. In addition, SSAO can sometimes create artifacts in the game world, such as halos around objects.\nFuture of SSAO Despite its limitations, SSAO is becoming an essential technique for game developers looking to create realistic graphics in their video games. As technology continues to improve, it is likely that SSAO will become more effective in simulating the way light behaves in real life. With the increasing demand for realistic graphics in video games, game developers will continue to rely on SSAO and other techniques to enhance the depth and realism of their games.\nConclusion Screen Space Ambient Occlusion is a technique that enhances the realism of video games by simulating the way light behaves in real life. SSAO works by analyzing the depth and normal information stored in the game\u0026rsquo;s frame buffer and calculating the amount of ambient light that should be reaching a pixel. The use of SSAO in video games has several benefits, including enhancing the realism and depth of the game world. While SSAO does have its limitations, it is becoming an essential technique for game developers looking to create realistic graphics in their video games. With its increasing popularity, it is important for game developers to continue to explore and experiment with SSAO to create even more realistic and immersive gaming experiences for players.\n闲言闲语 以上内容使用notion AI创作完成，刚好近期AIGC火的一塌糊涂，诸如Midjourney, Stable diffusion, 还有ChatGPT等等，以及很多大厂在GDC上面分享了由AI帮助生产的技术。我们也来凑个热闹好了。其实不难看出，AI在讲述一个完全未知效果的时候还挺好用的。一板一眼，条条框框，逻辑十分正确，至少比我强多了。而SSAO到底是什么，以及怎么实现，其实在上述内容已经讲的很清楚了。这里用中文列出来：\n获取屏幕空间深度 根据相机的世界位置，深度，就可以重建整个世界空间 URP下面没有法线图，构建法线的内容参考 https://wickedengine.net/2019/09/22/improved-normal-reconstruction-from-depth/\n在法线的方向的半球内随机撒点，点尽可能靠近像素中心位置。 构建像素点至随机点的向量，并转化至View 空间 对比depth 和构建的向量的深度，计算这个向量是否被遮挡。 加权平均半球内的全部随机点结果，取值缩放至 0 - 1范围内，即遮蔽效果的强度。 Reference https://www.yuque.com/yikejinyouzi/aau4tk/gs7n7d#u7t1x https://wickedengine.net/2019/09/22/improved-normal-reconstruction-from-depth/ https://atyuwen.github.io/posts/normal-reconstruction/ https://github.com/sebastianhein/urp-ssao/blob/master/Shaders/ssao.shader ","tags":["Unity","HLSL","C#","Render Feature"],"title":"URP屏幕空间环境光遮蔽"},{"categories":["学习笔记"],"date":"April 4, 2023","permalink":"https://kampter.github.io/blog/urp%E5%B1%8F%E5%B9%95%E7%A9%BA%E9%97%B4%E9%9B%BE%E6%95%88/","section":"blog","summary":"Unity URP Depth Fog Introduction Unity is a powerful game engine that has been used by game developers around the world to create high-quality games with various tools and features. One of these features is the Universal Rendering Pipeline (URP), which provides a lightweight rendering pipeline that is optimized for mobile and low-end hardware devices. In this blog post, we will discuss how to implement depth fog in Unity URP, which is a visual effect that creates a sense of depth and distance in a scene.\nWhat is Depth Fog? Depth fog is an atmospheric effect that makes distant objects appear hazy or blurry. This effect can be used to create a sense of scale in a scene and to make objects appear more realistic. Depth fog is a useful tool for game developers who want to create immersive environments that feel more natural.\nHow to Implement Depth Fog in Unity URP Unity URP provides a built-in Depth Texture that can be used to implement depth fog. The Depth Texture is a grayscale texture that represents the distance between the camera and the objects in the scene. To implement depth fog in Unity URP, follow these steps:\nCreate a new material and set its Shader to the URP/Lit shader. Enable the Depth Texture option in the material. Adjust the Depth Fade Start and Depth Fade Distance properties to control the distance at which the fog starts and how far it extends. Set the fog color in the material. Once you have created the material with the depth texture enabled, apply it to the objects in your scene that you want to be affected by depth fog. The objects closer to the camera will be less affected by the fog, while the objects further away will appear more hazy.\nBenefits of Using Depth Fog Depth fog is a powerful tool for game developers who want to create immersive environments. By using depth fog, developers can create a sense of depth and distance in a scene, which can make the scene feel more natural and realistic. Depth fog can also be used to create a sense of scale in a scene, which is important for games that involve large environments, such as open-world games.\nConclusion Depth fog is a useful visual effect that can be used to create a sense of depth and distance in a scene. Unity URP provides a built-in Depth Texture that can be used to implement depth fog. By following the steps outlined in this blog post, you can easily add depth fog to your Unity URP project. By using depth fog, game developers can create immersive environments that feel more natural and realistic, making for a more engaging gaming experience.\n技术总结 还是用 Notion AI帮忙生成一下对于Depth Fog的介绍。\n采样Depth 重建世界空间 三种雾效计算方式 这个系数在Unity内置雾效实现中，支持三种雾的计算方式——线性（Linear），指数（Exponential），指数的平方（Exponential Squared）。当给定距离z后。Linear的计算公式如下 $$ f;=;\\frac{d_{max};-;\\left|z\\right|}{d_{max};-;d_{ming}} $$ $d_{max}$和$d_{min}$分别表示受雾影响的最小距离和最大距离。\nExponential的计算公式如下 $$ f;=;e^{-d\\cdot\\left|z\\right|} $$ d是控制雾的浓度的参数。\nExponential Squared的计算公式如下 $$ f;=;e^{-\\left(d;\\cdot;\\left|z\\right|\\right)^2} $$\nhalf4 frag(Varyings input): SV_Target { // 采样depth 并且重建世界空间 half2 uv = input.positionCS.xy / _ScaledScreenParams.xy; float rawDepth = SampleSceneDepth(uv); float3 positionWS = ComputeWorldSpacePosition(uv, rawDepth, UNITY_MATRIX_I_VP); // 高度雾效系数 = （终止高度 - 当前像素高度）/（终止高度 - 起始高度） float fogDensity = (_FogEnd - positionWS.y) / (_FogEnd - _FogStart); // 计算浓度 saturate：截取到0-1 fogDensity = saturate(fogDensity * _FogDensity); // 原始颜色 half4 finalColor = SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, uv); // 插值原始颜色，与雾效颜色，使用雾效系数作为参数 finalColor.rgb = lerp(finalColor.rgb, _FogColor.rgb, fogDensity); return finalColor; } Reference https://www.lfzxb.top/unity-shader-userdepthandnormaltexture/ https://blog.csdn.net/puppet_master/article/details/77489948 ","tags":["Unity","HLSL","C#","Render Feature"],"title":"URP屏幕空间雾效"},{"categories":["学习笔记"],"date":"March 20, 2023","permalink":"https://kampter.github.io/blog/urp-14-renderfeature-%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F/","section":"blog","summary":"URP 14 URP 14 对应Unity 2022.2对于2021lts版本的URP 12中后处理的API有些许改变，需要参考官方文档的改变。而其中最大的改变来自于RTHandle的引入。\n以一个DesaturateFeature为例。在DesaturateFeature.cs中改变比较大的是加入一个SetupRenderPasses来帮助创建Materail和传递参数，而不是传统的写法在Create()中。\npublic override void SetupRenderPasses(ScriptableRenderer renderer, in RenderingData renderingData) { Material material = CoreUtils.CreateEngineMaterial(shader); desaturateRenderPass.ConfigureInput(ScriptableRenderPassInput.Color); desaturateRenderPass.Setup(material, \u0026#34;DesaturatedRenderPass\u0026#34;,renderingData); } 然后写内部的DesaturateRenderPass 类。\n变化一是RenderTargetHandle.Init(\u0026quot;_CustomPassHandle\u0026quot;)方式的改变为 var colorCopyDescriptor = renderingData.cameraData.cameraTargetDescriptor; colorCopyDescriptor.depthBufferBits = (int) DepthBits.None; RenderingUtils.ReAllocateIfNeeded(ref m_CopiedColor, colorCopyDescriptor, name: \u0026#34;_FullscreenPassColorCopy\u0026#34;); 变化二：command buffer的GetTemporaryRT和ReleaseTemporaryRT被弃用。\n官方推荐使用Blitter.BlitCameraTexture方法把原始的RT添加我们自定义的material然后再传递回去。代码实现如下\nCommandBuffer cmd = CommandBufferPool.Get(); var cameraData = renderingData.cameraData; using (new ProfilingScope(cmd, profilingSampler)) { var source =cameraData.renderer.cameraColorTargetHandle; passMaterial.SetTexture(m_BlitTextureShaderID, copiedColor); Blitter.BlitCameraTexture(cmd, source, copiedColor, passMaterial, 0); Blitter.BlitCameraTexture(cmd, copiedColor, source); context.ExecuteCommandBuffer(cmd); cmd.Clear(); } 参考官方的案例，完整写一个ColorTint的实例\nShader的部分可以十分简化，Vert在Blit.hlsl中官方已经帮忙处理好，这里只需要写frag的逻辑部分，太香了。\nShader \u0026#34;ColorBlit\u0026#34; { SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;RenderPipeline\u0026#34; = \u0026#34;UniversalPipeline\u0026#34;} LOD 100 ZWrite Off Cull Off Pass { Name \u0026#34;ColorTintPass\u0026#34; HLSLPROGRAM #include \u0026#34;Packages/com.unity.render-pipelines.core/Runtime/Utilities/BlitColorAndDepth.hlsl\u0026#34; #pragma vertex Vert #pragma fragment frag float4 _ColorTint; half4 frag (Varyings input) : SV_Target { half4 color = FragColorAndDepth(input).color; color *= _ColorTint; return color; } ENDHLSL } } } 完整的ColorTintRenderFeature部分。思路如下，用一个colorTintSettings做UI接住全部的设置内容。重写一个SetupRenderPasses来传递以前在Create()中传递的内容，创建就好好创建，设置参数就好好设置参数，嗯恒河里。RenderFeature class基本就这样没什么可以说的，与urp 12就这么点区别。哦对，在传递相机的部分，用renderer.cameraColorTargetHandle而不是cameraColorTarget。换句话说，unity 换了RThandle这个API，跟着换就行，虽然老的也能用，但是Unity官方论坛的讨论结果是未来RThandle会成为主流，你现在学会对未来没坏处。\n这里偷懒不想创建Volume，也不想多创建一个c#脚本来写colorTintPass，直接用一个intenal class来写吧。首先最大的区别就是用RThandle替代以前的API。重写OnCameraSetup()，调用ConfigureTarget()来设置当前Target，也就是刚刚传递过来的cameraColorTargetHandle。\nusing UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; public class ColorTintRenderFeature : ScriptableRendererFeature { [System.Serializable] public class ColorTintSettings { public Shader colorTintShader; public RenderPassEvent passEvent = RenderPassEvent.BeforeRenderingPostProcessing; public ScriptableRenderPassInput requirements = ScriptableRenderPassInput.Color; [Header(\u0026#34;ColorTint Settings\u0026#34;)] [Space(10)] public Color colorTint = Color.red; } public ColorTintSettings colorTintSettings = new ColorTintSettings(); private ColorTintPass colorTintPass; public override void SetupRenderPasses(ScriptableRenderer renderer, in RenderingData renderingData) { colorTintPass.ConfigureInput(colorTintSettings.requirements); colorTintPass.Setup(renderer.cameraColorTargetHandle, colorTintSettings); } public override void Create() { colorTintPass = new ColorTintPass(); colorTintPass.renderPassEvent = colorTintSettings.passEvent; } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { renderer.EnqueuePass(colorTintPass); } protected override void Dispose(bool disposing) { colorTintPass.Dispose(); } public class ColorTintPass : ScriptableRenderPass { private ProfilingSampler profilingSampler = new ProfilingSampler(nameof(ColorTintPass)); private ColorTintSettings colorTintSettings; private Material colorTintMaterial; private static readonly int ColorTintID = Shader.PropertyToID(\u0026#34;_ColorTint\u0026#34;); private static readonly int BlitTextureID = Shader.PropertyToID(\u0026#34;_BlitTexture\u0026#34;); private RTHandle source; private RTHandle copiedColor; public void Setup(RTHandle colorHandle, ColorTintSettings colorTintSettings) { this.colorTintSettings = colorTintSettings; Shader shader = colorTintSettings.colorTintShader; if (shader == null) return; colorTintMaterial = CoreUtils.CreateEngineMaterial(shader); source = colorHandle; } public void Dispose() { source.Release(); CoreUtils.Destroy(colorTintMaterial); } public override void OnCameraSetup(CommandBuffer cmd, ref RenderingData renderingData) { ConfigureTarget(source); var colorCopyDescriptor = renderingData.cameraData.cameraTargetDescriptor; colorCopyDescriptor.depthBufferBits = (int) DepthBits.None; RenderingUtils.ReAllocateIfNeeded(ref copiedColor, colorCopyDescriptor, name: \u0026#34;_FullscreenPassColorCopy\u0026#34;); } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { if (colorTintMaterial == null) return; CommandBuffer cmd = CommandBufferPool.Get(); var cameraData = renderingData.cameraData; using (new ProfilingScope(cmd, profilingSampler)) { colorTintMaterial.SetColor(ColorTintID, colorTintSettings.colorTint); Blitter.BlitCameraTexture(cmd, source, copiedColor); colorTintMaterial.SetTexture(BlitTextureID, copiedColor); Blitter.BlitCameraTexture(cmd, copiedColor, source, colorTintMaterial, 0); } context.ExecuteCommandBuffer(cmd); cmd.Clear(); CommandBufferPool.Release(cmd); } } } RT的初始化方法\nRenderingUtils.ReAllocateIfNeeded(ref copiedColor, colorCopyDescriptor, name: \u0026#34;_FullscreenPassColorCopy\u0026#34;); 这里偷懒直接用Blitter.BlitCameraTexture(cmd, source, source, colorTintMaterial, 0) 直接把source当作最终的RT并写入colorTintMaterial，按照逻辑的话，写成就和之前cmd.blit写出来到一个暂时的RT上再写入原source一样了。\nBlitter.BlitCameraTexture(cmd, source, temp, colorTintMaterial, 0); colorTintMaterial.SetColor(ColorTintID, colorTintSettings.colorTint); Blitter.BlitCameraTexture(cmd, temp, source); 最终效果如图\n参考 Upgrading to URP 14 (Unity 2022.2) | Universal RP | 14.0.6 (unity3d.com) ","tags":["HLSL","C#","Unity","Render Feature"],"title":"URP 14 RenderFeature 使用方式"},{"categories":["技术实现"],"date":"March 16, 2023","permalink":"https://kampter.github.io/blog/%E5%9C%A8urp%E4%B8%AD%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AAdisney-principled-brdf/","section":"blog","summary":"技术实现 Unity 自带的Lit 采用了通用的microfacet Cook-Torrance BRDF着色模型，对于各向异性，清漆，布料等支持并没有实现。这里希望搭建一个Disney Principled BRDF 来学习源码并且扩充这部分shader便于以后风格化定制。\nDiffuse项实现 half3 Disney_Diffuse(half3 diffuseColor, half roughness, half ndotV, half ndotL, half LdotH) { half FD90 = 0.5 + 2 * LdotH * LdotH * roughness; half FnV = SchlickFresnel(ndotV); half FnL = SchlickFresnel(ndotL); return lerp(1.0, FD90, FnV) * lerp(1.0, FD90, FnL); } unity实现如下，但是UE把LdotH修改成VdotH\nhalf3 Diffuse_Burley_Disney(float3 diffuseColor, float roughness, float ndotV, float ndotL, float LdotH) { float FD90 = 0.5 + 2 * LdotH * LdotH * roughness; float FdV = 1 + (FD90 - 1) * Pow5(1 - ndotV); float FdL = 1 + (FD90 - 1) * Pow5(1 - ndotL); return diffuseColor * ((1 / PI) * FdV * FdL); } UE的修改会造成远距离看diffuse模型变成一个没有明暗区分的模型。\n这与Disney的效果十分接近\n完整代码 偷懒没有逐一贴过来，完整写完直接代码在这里。\nShader \u0026#34;Custom/Disney PBR\u0026#34; { Properties { //Disney coefficients [MainTexture] _BaseMap (\u0026#34;Base Color\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _MainColor (\u0026#34;Color Tint\u0026#34;, Color) = (1.0, 1.0, 1.0, 1.0) _Metallic (\u0026#34;Metallic\u0026#34;, Range(0, 1)) = 0.0 _Smoothness (\u0026#34;Smoothness\u0026#34;, Range(0, 1)) = 0.5 _Subsurface (\u0026#34;Subsurface\u0026#34;, Range(0, 1)) = 0.0 _Specular (\u0026#34;Specular\u0026#34;, Range(0, 1)) = 0.5 _SpecularTint (\u0026#34;SpecularTint\u0026#34;, Range(0, 1)) = 0.0 _Anisotropic (\u0026#34;Anisotropic\u0026#34;, Range(0, 1)) = 0.0 _Sheen (\u0026#34;Sheen\u0026#34;, Range(0, 1)) = 0.0 _SheenTint (\u0026#34;SheenTint\u0026#34;, Range(0, 1)) = 0.5 _Clearcoat (\u0026#34;Clearcoat\u0026#34;, Range(0, 1)) = 0.0 _ClearcoatGloss (\u0026#34;ClearcoatGloss\u0026#34;, Range(0, 1)) = 0.0 //Normal control _BumpScale (\u0026#34;Normal Scale\u0026#34;, Range(0, 1)) = 1.0 _BumpMap (\u0026#34;Normal Map\u0026#34;, 2D) = \u0026#34;bump\u0026#34; {} //GI control // _IrradianceMap (\u0026#34;Irradiance Map\u0026#34;, CUBE) = \u0026#34;white\u0026#34; {} _LUT (\u0026#34;LUT\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} // _PrefilterMap (\u0026#34;Prefilter Map\u0026#34;, CUBE) = \u0026#34;white\u0026#34; {} // _AO (\u0026#34;AO\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;RenderPipeLine\u0026#34;=\u0026#34;UniversalRenderPipeline\u0026#34; \u0026#34;IgnoreProjector\u0026#34; = \u0026#34;True\u0026#34; } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; TEXTURE2D(_BaseMap); SAMPLER(sample_BaseMap); TEXTURE2D(_BumpMap); SAMPLER(sampler_BumpMap); TEXTURE2D(_LUT); SAMPLER(sampler_LUT); CBUFFER_START(UnityPerMaterial) half4 _BaseMap_ST; half4 _MainColor; half _Metallic; half _Smoothness; half _Subsurface; half _Specular; half _SpecularTint; half _Anisotropic; half _Sheen; half _SheenTint; half _Clearcoat; half _ClearcoatGloss; half4 _BumpMap_ST; half _BumpScale; half4 _LUT_ST; CBUFFER_END struct Attributes { half4 positionOS : POSITION; half3 normalOS : NORMAL; half4 tangetOS : TANGENT; half2 uv : TEXCOORD0; half2 lightmapUV : TEXCOORD1; UNITY_VERTEX_INPUT_INSTANCE_ID }; struct Varyings { half4 positionCS : SV_POSITION; half3 positionWS : TEXCOORD0; half3 normalWS : TEXCOORD1; half2 uv : TEXCOORD2; half3 tangentWS : TEXCOORD3; half2 lightmapUV : TEXCOORD4; half3 bitangentWS : TEXCOORD5; half4 viewDirWS : TEXCOORD6; half fogCoord : TEXCOORD7; UNITY_VERTEX_INPUT_INSTANCE_ID }; ENDHLSL Pass { Name \u0026#34;DisneyForward\u0026#34; Tags{ \u0026#34;LightMode\u0026#34;=\u0026#34;UniversalForward\u0026#34; } CULL OFF HLSLPROGRAM #pragma vertex vert #pragma fragment frag #pragma multi_compile_fog #pragma shader_feature_local_fragement _EMISSION #define _NORMALMAP Varyings vert(Attributes IN) { Varyings OUT = (Varyings)0; UNITY_SETUP_INSTANCE_ID(IN); UNITY_TRANSFER_INSTANCE_ID(IN, OUT); const VertexPositionInputs vpi = GetVertexPositionInputs(IN.positionOS); const VertexNormalInputs vni = GetVertexNormalInputs(IN.normalOS, IN.tangetOS); // get positive or negative normal signal (should be either 1 or -1) half sign = IN.tangetOS.w * GetOddNegativeScale(); OUT.positionWS = vpi.positionWS; OUT.positionCS = vpi.positionCS; OUT.uv = TRANSFORM_TEX(IN.uv, _BaseMap); OUT.normalWS = vni.normalWS; OUT.tangentWS = vni.tangentWS; OUT.bitangentWS = vni.bitangentWS; OUT.lightmapUV = IN.lightmapUV; OUT.fogCoord = ComputeFogFactor(OUT.positionCS.z); return OUT; } half Pow2(half v) { return v * v; } half Pow5(half v) { return v * v * v * v * v; } half SchlickFresnel(half v) { v = clamp(1 - v, 0, 1); return Pow5(v); } half3 Disney_Diffuse_Kfd(half roughness, half ndotV, half ndotL, half LdotH) { half FD90 = 0.5 + 2 * Pow2(LdotH) * roughness; half FnV = SchlickFresnel(ndotV); half FnL = SchlickFresnel(ndotL); return lerp(1.0, FD90, FnV) * lerp(1.0, FD90, FnL); } half Disney_Specular_GTR2_iso(half NdotH, half roughness) { half a2 = Pow2(roughness); half den = 1.0 + (a2 - 1.0) * Pow2(NdotH); return a2 / (Pow2(den) * PI); } half SmithsG_GGX_aniso(half NdotV, half XdotV, half YdotV, half ax, half ay) { return 1 / (NdotV + sqrt(Pow2(XdotV * ax) + Pow2(YdotV * ay) + Pow2(NdotV))); } half3 Specular_Fresnel(half3 Ctint, half3 Cdlin, half LdotH, half Metallic) { half FlH = SchlickFresnel(LdotH); half3 F0 = lerp(_Specular * 0.08 * lerp(half3(1, 1, 1), Ctint, _SpecularTint), Cdlin, Metallic); half3 F = lerp(F0, half3(1, 1, 1), FlH); return F; } half Disney_Clear_GTR1(half NdotH, half a) { if (a \u0026gt;= 1) { return 1 / PI; } float a2 = Pow2(a); return (a2 - 1) / (log(a2) * (1 + (a2 - 1) * Pow2(NdotH)) * PI); } half Disney_Clear_GGX(half NdotV, half NdotL, half a) { half a2 = Pow2(a); half GGXnv = 1 / (NdotV + sqrt(a2 + (1 - a2) * Pow2(NdotV))); half GGXnl = 1 / (NdotL + sqrt(a2 + (1 - a2) * Pow2(NdotL))); return GGXnv * GGXnl; } half Disney_Subsurface_ss(half roughness, half LdotH, half NdotL, half NdotV) { half FnL = SchlickFresnel(NdotL); half Fnv = SchlickFresnel(NdotV); half Fss90 = Pow2(LdotH) * roughness; half Fss = lerp(1.0, Fss90, FnL) * lerp(1.0, Fss90, Fnv); return 1.25 * (Fss * (1 / (NdotV + NdotL) - 0.5) + 0.5); } half3 fresnelSchlickRoughness(half cosTheta, half3 F0, half roughness) { return F0 + (max(half3(1.0 - roughness, 1.0 - roughness, 1.0 - roughness), F0) - F0) * Pow5(1.0 - cosTheta); } half4 frag(Varyings IN) : SV_Target { //albedo half4 albedoAlpha = SAMPLE_TEXTURE2D(_BaseMap, sampler_BumpMap, IN.uv); half3 albedo = albedoAlpha.rgb * _MainColor.rgb; half alpha = albedoAlpha.a * _MainColor.a; //rip off the energy from albedo float Cdlum = 0.3 * albedo.r + 0.6 * albedo.g + 0.1 * albedo.b; float3 Ctint = Cdlum \u0026gt; 0 ? (albedo / Cdlum) : float3(1, 1, 1); float3 Csheen = lerp(float3(1, 1, 1), Ctint, _SheenTint); //normal #ifdef _NORMALMAP half3 normalTS = UnpackNormalScale(SAMPLE_TEXTURE2D(_BumpMap, sampler_BumpMap, IN.uv), _BumpScale); IN.normalWS = TransformTangentToWorld(normalTS, half3x3(IN.tangentWS.xyz, IN.bitangentWS.xyz, IN.normalWS.xyz)); #endif //roughness float perceptualRoughness = 1.0 - _Smoothness; float roughness = perceptualRoughness * perceptualRoughness; float squareRoughness = roughness * roughness; // Direction Function half3 viewDirWS = GetWorldSpaceViewDir(IN.positionWS); // Lighting Calculation half4 shadowCoord = TransformWorldToShadowCoord(IN.positionWS); Light mainLight = GetMainLight(shadowCoord); half3 mainLightDir = normalize(TransformObjectToWorldDir(mainLight.direction)); //anisotropic half3 halfVector = normalize(viewDirWS + mainLightDir); half aspect = sqrt(1.0 - _Anisotropic * 0.9); half ax = max(0.001, squareRoughness / aspect); half ay = max(0.001, squareRoughness * aspect); half hx = max(saturate(dot(halfVector, IN.tangentWS)), 0.000001); half hy = max(saturate(dot(halfVector, IN.bitangentWS)), 0.000001); half XdotV = dot(hx, viewDirWS); half YdotV = dot(hy, viewDirWS); half XdotL = dot(hx, mainLightDir); half YdotL = dot(hy, mainLightDir); // Dot Product Function half NdotL = max(0.000001, saturate(dot(IN.normalWS, mainLightDir))); half NdotV = max(0.000001, saturate(dot(IN.normalWS, viewDirWS))); half NdotH = max(0.000001, saturate(dot(IN.normalWS, halfVector))); half VdotH = max(0.000001, saturate(dot(viewDirWS, halfVector))); half LdotH = max(0.000001, saturate(dot(mainLightDir, halfVector))); //directLight Specular //D term half Ds = Disney_Specular_GTR2_iso(NdotH, roughness); //G term half GnV = SmithsG_GGX_aniso(NdotV, XdotV, YdotV, ax, ay); half GnL = SmithsG_GGX_aniso(NdotL, XdotL, YdotL, ax, ay); half Gs = GnV * GnL; //F term half3 Fs = Specular_Fresnel(Ctint, albedo, LdotH, _Metallic); //directLight clearcoat D F G half Dr = Disney_Clear_GTR1(NdotH, lerp(0.1, 0.001, _ClearcoatGloss)); half3 F0 = float3(0.04, 0.04, 0.04); half3 Fr = lerp(F0, float3(1, 1, 1), SchlickFresnel(LdotH)); half Gr = Disney_Clear_GGX(NdotV, NdotL, 0.25); //conbine specular part half3 specular = Gs * Fs * Ds + Dr * Gr * Fr * 0.25 * _Clearcoat; // diffuse half3 Kd = Disney_Diffuse_Kfd(roughness, NdotV, NdotL, VdotH); half ss = Disney_Subsurface_ss(roughness, LdotH, NdotL, NdotV); half3 Fsheen = SchlickFresnel(LdotH) * _Sheen * Csheen; half3 diffuse = (albedo * lerp(Kd, ss, _Subsurface) /PI + Fsheen) * (1.0 - _Metallic); // direct Lighting half3 directLight = (diffuse + specular) * mainLight.color * NdotL; directLight *= mainLight.distanceAttenuation; // indirect Lighting //GI Diffuse F0 = lerp(kDieletricSpec.rgb, albedo, _Metallic); half3 F_ibl = fresnelSchlickRoughness(NdotV, F0, roughness); half kd_ibl = (1 - F_ibl.r) * (1 - _Metallic); half3 irradiance = SampleSH(IN.normalWS); half3 inDiffuse = kd_ibl * albedo * irradiance; //GI Specular half3 reflectVector = reflect(-viewDirWS, IN.normalWS); half mip = roughness * (1.7 - 0.7 * roughness) * UNITY_SPECCUBE_LOD_STEPS; half3 prefilter_Specular = SAMPLE_TEXTURECUBE_LOD(unity_SpecCube0, samplerunity_SpecCube0, reflectVector, mip); half2 envBRDF = SAMPLE_TEXTURE2D(_LUT, sampler_LUT, float2(lerp(0, 0.99, NdotV), lerp(0, 0.99, roughness))).rg; half3 inSpecular = prefilter_Specular * (envBRDF.r * F_ibl + envBRDF.g); half3 indirectLight = inDiffuse + inSpecular; half4 color = half4(directLight + indirectLight , 1); // apply fog color.xyz = MixFog(color.xyz, IN.fogCoord); return color; } ENDHLSL } } } 间接光漫反射的部分偷懒直接使用Unity URP内置的 sampleSH函数。而间接光镜面反射部分也直接用reflection probe作为prefileter 采样。envBRDF的lut贴图直接采样预计算好的。其实如果仔细看源码的部分URP在间接光照的部分也对clearcoat进行处理\n最终效果 我实现的效果在左边，对比右边URP下默认的Lit\n","tags":["PBR Shading","Unity","HLSL"],"title":"在URP中搭建一个Disney Principled BRDF"},{"categories":["课程笔记"],"date":"February 26, 2023","permalink":"https://kampter.github.io/blog/games-202-pbr-rendering-%E6%80%BB%E7%BB%93/","section":"blog","summary":"直接光照 在不考虑能量补偿的情况下 传统的Blinn Phong模型是\ndiffuse + specular + ambient\n而cook-torrance BRDF的方程是\n其中Ks = F, 且金属材质的漫反射为0\n阴影 各种优化过的Shadow Map, PCF, PCSS等等\n间接光照 间接光照与直接光照思路相同\n直接光源是指物体自身发出的光，比如灯泡、蜡烛、荧光棒、太阳 间接光源是指物体反射出来的光，直接看图，下边的右图，两边的墙壁反射出自身的颜色，照亮了场景中的物体 那么间接光照的实现思路也是\n有哪些间接光源会向外发射Radiance？（找到光源） 哪些方向的间接光源Radiance会到达着色点？（构建光照模型） 环境光 环境光包括其他物体反射的光以及所谓天空盒发出的光照\n最简单的环境光可以是一个常量\nfloat3 ambient = float3(0.1, 0.1, 0.1); 而符合物理的计算方式是基于图像的照明（IBL）来计算\n漫反射部分 而Irradiance因为是低频信号可以用SH函数拟合。\n其中使用PRT改进的SH就自带类似AO的效果。\n镜面反射比较难实现 SSR是基于光线追踪的做法\n阴影 间接光照的阴影实现比较难实现，目前较为黑科技的代表是SSAO\n能量补偿部分 后面有机会实现一下\n参考 重新理解PBR（1） - 知乎 (zhihu.com) 重新理解PBR（2）——漫反射全局光照 - 知乎 (zhihu.com) PBR渲染: Cook-Torrance的实现与补充 | Blurred code ","tags":["Computer Graphic","Math","Games 202"],"title":"Games 202 - PBR rendering 总结"},{"categories":["课程笔记"],"date":"February 25, 2023","permalink":"https://kampter.github.io/blog/games-202-pbr%E6%9D%90%E8%B4%A8/","section":"blog","summary":"PBR PBR包括Materials, lighting, camera, light transport等等任何与渲染有关的基于物理的内容。而工业界习惯叫PBR都是指PBR材质，虽然之前已经在unity中看过URP中PBR的实现原理并且尝试手动还原，这次看过Games 202后还是更加深化对PBR材质的了解。\n基于表面的材质\nMicrofacet models微表面模型（不是完全基于物理的）\nDisney Principled BRDF能够用于离线渲染, 但也可以运用在实时渲染中，但也不是PBR，是基于artist的角度来考虑的。\n基于体积的材质\n实时渲染中并没有什么特别好的解决方案，常见的诸如云，头发，皮肤。 Microfacet BRDF F项：菲涅尔项，表示观察角度与反射的关系(从一个角度看去会有多少的能量被反射) 当视线与反射表面夹角越小，反射越明显。水体是菲涅尔效应最明显的现实物体之一（当站在湖边看到脚下的湖水是透明的，而远处湖面的水则是不透明的，并且反射非常强烈）。\n标准菲尼尔项计算公式：\n而工业界主流使用采用 Schlick 的 Fresnel 近似，因为计算成本低廉，而且精度也足够。其中，n1、n2分别为两种介质的折射率。一般假设 n1=1 近似于空气折射率，而 n2取决于被渲染的物体介质。\nD项：微表面的法线分布决定这一项的是不同微表面朝向的法线分布；简单来说，当微平面的法线分布比较集中（各法线朝向大致相同）时，那么物体表面材质会更容易表现出高光；当微平面的法线分布比较散开（各法线朝向差异比较大）时，那么物体表面材质将表现的非常 diffuse。\n传统的 Blinn-Phong 模型也是法线分布模型\nBeckmann NDF 是第一批微平面模型中使用的法线分布，也是 Cook-Torrance BRDF 在最初提出时选择的NDF。其中，α∈[0,1] 表示为表面的粗糙程度。\nGGX/Trowbridge-Reitz NDF 是URP现在采用的法线分布函数\nGGX / Trowbridge-Reitz NDF 与 Beckmann NDF 的主要区别在于前者函数具有更长的尾巴，这样就可以让高光部分过渡部分更加缓和，从而更加自然。\nGeneralized-Trowbridge-Reitz（GTR）\n其中， γ参数用于控制尾部形状。 当 γ=2 时，GTR等同于GGX。 随着 γ的值减小，分布的尾部变得更长。而随着 γ 值的增加，分布的尾部变得更短。\nG项：几何函数体现了光在物体微平面上反射时的损耗，一般指两种损耗：阴影（Shadowing）和遮蔽（Obstruction）。\n阴影来自光照射在微表面的遮挡，而遮蔽来自光反弹后被微表面遮挡。\n能量补偿项（Energy Compensation Term） 几何函数表示了光在物体微平面上反射时的损耗，但实际上这些光线并不是损耗了，而是变成了微平面之间的互反射或多次表面反射的光线，但是Microfacet理论忽略了这些反射，这样做的缺点是会造成越 diffuse 的物体能量损失越多，从而使粗糙物体渲染偏暗。\n解决办法是使用 The Kulla-Conty Approximation 补偿能量在BRDF上面\n使用能量补偿后渲染方程变为\n闫神说现在有人用diffuse + brdf 来描述能量补偿，也就是粗暴的用diffuse来补偿能量，但是不符合能量守恒定律。\n这里的补偿是指单独对specular BRDF的补偿，而不包括diffuse部分的，因此所谓弹幕中说opengl中cook-torrance brdf多加diffuse错误的理解是不对的。闫神指出这种错误其实在最终计算会叠加2次diffuse因此是能量不守恒的。\nLinearly Transformed Cosines (LTC) LTC:在不考虑遮挡和阴影情况下做微表面模型的shading.\n我的理解是为了实现多边形光源对原Microfacet BRDF的改进，主要改进GGX发现分布项。\n但是这个模型并不能很好的支持阴影\nDisney’s Principled BRDF Disney 总体上采用 microfacet Cook-Torrance BRDF 着色模型，并对其中一些项进行了改造，总体公式如下：\n参考Disney 源码：\n漫反射（Diffuse）\n高光部分与Cook-Torrance BRDF类似\n菲涅尔项（Fresnel Term）\n法线分布项（Normal Distribution Term）\n使用了 Generalized-Trowbridge-Reitz（GTR）γ=2 的版本，并根据各向异性做了一定改进。anisotropicanisotropic 表述了各向异性的程度，Xa,Ya则描述了各向异性的旋转方向。\n几何函数 （Geometry Function）Disney 在几何函数上使用 Smith 遮蔽函数，并基于各向异性 GGX 分布来得到如下：\n次表面散射SSS\nSheen 布料光泽\n清漆项（Clearcoat）简单粗暴的使用cook-torrance高光 * clearcoat\n使用 γ=1的各向同性 GTR 法线分布，并通过 clearcoatGlossclearcoatGloss 来控制清漆的光滑程度：\n使用 f0 = 0.04 的菲尼尔项和使用a = 0.25的ggx各向同性GGX\n\u0026mdash; 2023.03.16 更新\u0026mdash;\n在URP中搭建一个Disney Principled BRDF\n","tags":["Computer Graphic","Math","Games 202"],"title":"Games 202 - PBR材质"},{"categories":["日常记录"],"date":"February 15, 2023","permalink":"https://kampter.github.io/blog/global-game-jam-2023%E5%9B%9E%E9%A1%BE/","section":"blog","summary":"心得 抱歉这么晚才来写这个回顾，近期忙于过年，情人节，以及学习等等事项，年后到现在终于有一段时间相对比较安静可以回顾一下这个赛事。\n首先感谢GGJ 2023能在这个时期召开，我从前公司离职到现在学习游戏制作已经有半年多的时间。一直在学习但是没有什么机会能够回顾学会什么，还欠缺什么，还想要什么。而且在这种比赛的压力下，很多问题才会暴漏出来，感谢这次能够让我总结的机会。\n其次感谢我的组员：\n48小时开发辛苦了，组内的除了我之外都是已经在游戏公司内工作的大佬，很高兴能与你们一起合作完成这个作品，希望我没有给你们拖后腿。\n最后感谢我的女朋友，感谢她在开发前与我一起准备；开发中陪伴我并鼓励，帮我一起寻找灵感；结束后我们一起分享喜悦。她也喜欢游戏，也喜欢宫崎骏的动画。游戏场景内的部分渲染风格也是我的一点私心，希望未来有机会能构建一个独属于我们的世界。\n技术实现 鉴于这个48小时的小项目，虽然之前一直在玩HDRP渲染管线。这次就老老实实用URP了，这也是我第一次用URP。之前在HDRP中大部分shader都用shader graph就可以完成，这次在urp中纯手写还是很有挑战性的。\n整体美术风格，希望尝试一下卡通渲染。场景渲染希望借鉴吉卜力风格治愈、唯美的感觉，同时借鉴宫崎骏动画中对自然和平的追求。而人物我们选择格鲁特这一经典漫威角色，我想尝试一下厚涂风格带来的立体感。\n找参考 背景参考，可以说是整个流程里最轻松的部分。因为我需要做的就是去看原来知名的对应题材的作品，游戏，动画，影视等。寻找其中的共同背景，色彩构成，空间元素构成。\n乱糟糟随心所欲找了一些东西，也没有成体系。理想的过程是从图片总结出风格特点，再转化成引擎可以实现的方案做出来。\n基础场景搭建 刚好在2022年底花不少精力学习和排坑的houdini PDG流程这次刚好可以应该用在这次项目中。\n用houdini 生成Heightmap已经是非常轻松愉快的事情了，这个项目不需要高精度雕琢，简单调整一些细节就可以搭建PDG节点了。哦对，在这之前要在unity中准备好terrain layer以及通过随即撒点种植的植物prefab，同时设置好LOD。\n大致流程就是在houdini中获取unity中准备好的prefab path，导出至foliage.csv中，然后把unity_instance写入prefab path属性再导入unity中cook。\n用attributewrangle读取csv文件中的路径并写入撒点的s@unity_instance属性中，这一步操作很像之前在jupyter中操作csv文件。可以加一步将position, normal, orient, pscale等属性导出Pcache，通过VFX graph制作可交互的草地（官方演示Demo）这次没有使用。\n成品效果大致是这样，忽略这个很像ue的场景，是ECS2 input system自带的。\n###草地渲染\n目前我比较喜欢的有三种方式来制作草地。\n第一种是terrain笔刷刷上带LOD的prefab模型，这是渲染效果最好的方案，也是成本最高的。 第二种是terrain笔刷刷上带LOD的prefab插片草地，效果对比第一种略显粗糙但是性能提高很多。 第三种来自官方的GrassWind Demo，在VFX graph中使用刚刚houdini导出的Pcache，而模型的部分可以任选模型或者插片，只是性能上的区别。 这次方案我们选择了第二种，带LOD的插片草地。插片草地的制作也很简单，在houdini中简单调整一下位置，垂直相机拍摄出texture即可。注意传递球形法线并且只保留向上的部分。\n然后用triangulate2d 勾勒出边界并且组合成插片草模型。草地模型导入unity后关闭投射阴影，降低阴影带来的杂乱感。\n草地渲染shader 我这次使用shader graph来制作，感谢2021.3版本的urp中支持shader graph。\n把草从上到下用top color和bottom color lerp起来控制主体，随后叠加两层gradient noise从整体控制草地的颜色而不仅仅是单一模型的变化同时模拟草地整体阴影效果。Wind的vertex movment也是同理，但是这里偷懒没有做，后面有机会去实现一下。\n树木渲染 树干的部分用half lambert 和blinn specular实现。\n// diffuse half4 albedo = SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, IN.uv) * _BaseColor; // specular half _smoothness = SAMPLE_TEXTURE2D(_SmoothTex, sampler_SmoothTex, IN.uv).g; half3 specular = pow(ndotH, _smoothness) * lightColor; // lambert half lambert = ndotL; half halfLambert = saturate(lambert * 0.5 + 0.5); half colorLambert = halfLambert * _ColorLambertFactor + 1 - _ColorLambertFactor; float3 color = colorLambert * albedo + specular; 树叶也使用插片树叶实现，shader部分略微复杂，除了half lambert 和blinn specular以外。计算树叶自阴影增加细节。\nhalf4 shadowWS = TransformWorldToShadowCoord(IN.positionWS); half shadowAttenuation = GetMainLight(shadowWS).shadowAttenuation; color = color * lerp(1, shadowAttenuation, _ShadowIntensity); 模拟投射效果，很大程度上提升树叶颜色的纯度。\nfloat3 transmissionhalf = normalWS + normalize(lightDir); float3 transmission = saturate(dot(-transmissionhalf, viewDirWS) - _TransmissionThreshold); transmission = lerp( 1 + 0.5 * _TransmissionIntensity, 1 +_TransmissionIntensity, transmission); color = color * transmission; 模拟假的SSS效果\nhalf3 step1 = -1 * saturate((normalWS * _NormalInfluence * lightDir)); half4 step2 = pow(saturate(dot(step1, viewDirWS)), _SSSPower) * _SSSColor; half4 step3 = (1 - _Thickness) * step2;color = color + step3; 最后的效果图如下，踩坑点，记得写阴影pass并且阴影也要alpha clip。还是比较杂乱，没有把模型导入houdini传递一下球形法线，48小时时间来不及了。\nPass { Name \u0026#34;Depth Rim\u0026#34; Tags {\u0026#34;LightMode\u0026#34; = \u0026#34;DepthOnly\u0026#34;} ZWrite On ColorMask 0 Cull Off HLSLPROGRAM #pragma vertex DepthOnlyVertex #pragma fragment DepthOnlyFragment #include \u0026#34;Packages/com.unity.render-pipelines.universal/Shaders/LitInput.hlsl\u0026#34; TEXTURE2D(_MainTex); SAMPLER(sampler_MainTex); CBUFFER_START(UnityPerMaterial) half4 _MainTex_ST; half _ClipValue; CBUFFER_END struct Attributes { float4 positionOS : POSITION; float2 uv : TEXCOORD0; UNITY_VERTEX_INPUT_INSTANCE_ID }; struct Varyings { float2 uv : TEXCOORD0; float4 positionCS : SV_POSITION; UNITY_VERTEX_INPUT_INSTANCE_ID UNITY_VERTEX_OUTPUT_STEREO }; Varyings DepthOnlyVertex(Attributes IN) { const VertexPositionInputs vertex_position_inputs = GetVertexPositionInputs(IN.positionOS); Varyings OUT; OUT.positionCS = vertex_position_inputs.positionCS; OUT.uv = TRANSFORM_TEX(IN.uv, _MainTex);; return OUT; } half4 DepthOnlyFragment(Varyings IN) : SV_TARGET { UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); half4 diffColor = SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, IN.uv); clip(diffColor - _ClipValue); return 0; } ENDHLSL } 人物渲染 人物渲染我用的是shader graph + custom function的方式实现。\n大致思路与直接手写没有太大区别。首先遍历场景内的灯光（多灯光支持）获得每个灯光的颜色，方向，衰减属性用以叠加计算。光照模型用smoothstep函数控制半兰伯特模型的输出，同时设置Smooth参数来调整边界的光滑度。\n最左边smooth = 0, 中间smooth = 0.2 右边为开启边缘光的效果。边缘光用非常偷懒的方式实现\n水面渲染 用根据时间变化的gradient noise控制水面的vertex position 移动，主要用以实现小幅度的潮汐的感觉。\n水面颜色用屏幕空间深度来控制，lerp深水区和浅水区颜色，这个实现来自unity的官方演示视频。\n通过调整Strength和depth控制浅水和深水区\n反射和水面波纹实现。选取两张水面法线纹理图，设置好根据时间移动的方向和速度，用reoriented normal blend的叠加方式（效果更好），用以模拟水面被风吹动的波纹。水面反射通过把刚刚的结果叠加至Screen position中，再把结果输出至Scene Color的UV中, 与给刚刚求得的深度图结果根据aplha通道进行lerp，输出至normal即可获得还算不错的具有反射效果的水面波纹。\n最终效果如图\n体积云 云直接用的cubemap贴图贴上去的skybox，48小时实在没有时间实现了\n总结 不少内容都是临时拼凑和网上抄来的，也没有认真调整和细化方案，没有定制自己的东西。感觉还是太菜了，希望接下来继续沉下来多积累一些东西。\n参考 风格化树——树叶分析与实现 - 知乎 (zhihu.com)\n水一下原神树的渲染 - 知乎 (zhihu.com)\nMaking a Water Shader in Unity with URP! (Tutorial) - YouTube\n(4) Unity | Making a Lit Toon Shader in Shader Graph - YouTube\n","tags":["Shader Graph","HLSL","Houdini","Unity","Toon Shading"],"title":"GGJ2023 x GiCA 中国站回顾"},{"categories":["技术实现"],"date":"February 10, 2023","permalink":"https://kampter.github.io/blog/%E5%B0%9D%E8%AF%95%E5%9C%A8urp%E4%B8%AD%E6%89%8B%E5%86%99-pbr-shader/","section":"blog","summary":"Cook-Torrance BRDF Cook-Torrance BRDF的镜面反射部分包含三个函数，此外分母部分还有一个标准化因子 。字母D，F与G分别代表着一种类型的函数，各个函数分别用来近似的计算出表面反射特性的一个特定部分。三个函数分别为法线分布函数(Normal Distribution Function)，菲涅尔方程(Fresnel Rquation)和几何函数(Geometry Function)：\nD项 - 法线分布函数: 估算在受到表面粗糙度的影响下，朝向方向与半程向量一致的微平面的数量。这是用来估算微平面的主要函数。\nhalf Function_D(half ndotH, half roughness) { half a2 = roughness * roughness; half ndotH2 = ndotH * ndotH; half nom = a2; half denom = (ndotH2 * (a2 - 1.0) + 1.0); denom = PI * denom * denom; return nom/denom; } G项 - 几何函数：\n几何函数从统计学上近似的求得了微平面间相互遮蔽的比率，这种相互遮蔽会损耗光线的能量。\n为了有效的估算几何部分，需要将观察方向（几何遮蔽(Geometry Obstruction)）和光线方向向量（几何阴影(Geometry Shadowing)）都考虑进去。我们可以使用史密斯法(Smith’s method)来把两者都纳入其中(第二次计算把v换成l)：\n这里的k是α的重映射(Remapping)，取决于我们要用的是针对直接光照还是针对IBL光照的几何函数:\nhalf GeometrySchlickGGX(half ndotV, half k) { float nom = ndotV; float denom = ndotV * (1.0 - k) + k; return nom / denom; } half GeometrySmith(half ndotV, half ndotL, half k) { float ggx1 = GeometrySchlickGGX(ndotV, k); float ggx2 = GeometrySchlickGGX(ndotL, k); return ggx1 * ggx2; } half Function_G(half ndotV, half ndotL, half k) { return GeometrySmith(ndotV, ndotL, k); } F项 - 菲涅尔方程：菲涅尔方程描述的是在不同的表面角下表面所反射的光线所占的比率。 菲涅尔方程是一个相当复杂的方程式，不过幸运的是菲涅尔方程可以用Fresnel-Schlick近似法求得近似解：\nF0表示平面的基础反射率，它是利用所谓折射指数(Indices of Refraction)或者说IOR计算得出的。然后正如你可以从球体表面看到的那样，我们越是朝球面掠角的方向上看（此时视线和表面法线的夹角接近90度）菲涅尔现象就越明显，反光就越强：\n// calculate F0 half3 Calculate_F0(half3 albedo, half metalness) { half3 F0 = 0.04; F0 = lerp(F0, albedo, metalness); return F0; } half3 Function_F(half ndotV, half3 albedo, half metalness) { half3 F0 = Calculate_F0(albedo, metalness); return F0 + (1.0 - F0) * pow(1.0 - ndotV, 5.0); } 这里的kd是早先提到过的入射光线中被折射部分的能量所占的比率，而ks是被反射部分的比率。BRDF的左侧表示的是漫反射部分，它被称为Lambertian漫反射，这和我们之前在漫反射着色中使用的常数因子类似，用如下的公式来表示：\nBRDF的镜面反射部分要稍微更高级一些，它的形式如下所示：\n// direct specular half smoothness = 1; half roughness = 1 - smoothness; half functionD = Function_D(ndotH, roughness); // remap roughness to k value half kDirect = pow(roughness + 1, 2) / 8; half direct_functionG = Function_G(ndotV, ndotL, kDirect); half3 functionF = Function_F(ndotV, albedo, metalness); half3 nominator = functionD * direct_functionG * functionF; half demoninator = 4.0 * ndotV * ndotL + 0.001; half3 specular = nominator / demoninator; 实时渲染中几乎只能使用Cook-Torrance BRDF模型。BRDF同时描绘入射光线和出射光线。严格上来说，Blinn-Phong光照模型也被认为是一个BRDF。然而由于Blinn-Phong模型并没有遵循能量守恒定律，因此它不被认为是基于物理的渲染。BRDF兼有漫反射和镜面反射两个部分：\n最终渲染方程：\n// Direct BRDF = Lambertian + Direct Specular // direct specular half smoothness = 1; half roughness = 1 - smoothness; half functionD = Function_D(ndotH, roughness); // remap roughness to k value half kDirect = pow(roughness + 1, 2) / 8; half direct_functionG = Function_G(ndotV, ndotL, kDirect); half3 functionF = Function_F(ndotV, albedo, metalness); half3 nominator = functionD * direct_functionG * functionF; half demoninator = 4.0 * ndotV * ndotL + 0.001; half3 specular = nominator / demoninator; // Lambertian half3 kS = functionF; half3 kD = half3(1.0, 1.0, 1.0) - kS; kD *= 1.0 - metalness; // Render Equation half3 radiance = mainLightColor * mainLightAtten; half3 Lo = (kD * albedo / PI + kS * specular) * radiance * ndotL * AO; half3 directColor = ambient + Lo; Image based lighting IBL 通常使用（取自现实世界或从3D场景生成的）环境立方体贴图 (Cubemap) ，我们可以将立方体贴图的每个像素视为光源，在渲染方程中直接使用它。这种方式可以有效地捕捉环境的全局光照和氛围，使物体更好地融入其环境。\n由于基于图像的光照算法会捕捉部分甚至全部的环境光照，通常认为它是一种更精确的环境光照输入格式，甚至也可以说是一种全局光照的粗略近似。基于此特性，IBL 对 PBR 很有意义，因为当我们将环境光纳入计算之后，物体在物理方面看起来会更加准确。\n间接Diffuse 仔细观察漫反射积分，我们发现漫反射兰伯特项是一个常数项（颜色 c 、折射率 kd 和 π 在整个积分是常数），不依赖于任何积分变量。基于此，我们可以将常数项移出漫反射积分：\n这边没有直接参考公式手写轮子，而是摘抄Unity在SphericalHarmonics.hlsl中的实现代码，原理相同，都是对环境光cubemap进行采样。\nreal3 SH_indirectDiffuse(real3 normalWS, real AO) { real4 SHCoefficients[7]; SHCoefficients[0] = unity_SHAr; SHCoefficients[1] = unity_SHAg; SHCoefficients[2] = unity_SHAb; SHCoefficients[3] = unity_SHBr; SHCoefficients[4] = unity_SHBg; SHCoefficients[5] = unity_SHBb; SHCoefficients[6] = unity_SHC; real3 color = SampleSH9(SHCoefficients, normalWS); return max(0, color) * AO; } 间接Specular 间接高光部分的代码分别参考ImageBasedLighting.hlsl, EntityLighing.hlsl, GlobalIllumination.hlsl的代码实现，用了比较偷懒的方式而不是完全手写。\nhalf3 IndirectSpecular(half reflectVector, half roughness, half AO) { half mip = PerceptualRoughnessToMipmapLevel(roughness); half4 encodedIrradiance = half4(SAMPLE_TEXTURECUBE_LOD(unity_SpecCube0, samplerunity_SpecCube0, reflectVector, mip)); half3 irradiance = DecodeHDREnvironment(encodedIrradiance, unity_SpecCube0_HDR); return irradiance * AO; } // Indirect Function F half3 EnvironmentBRDFSpecular(half roughness, half3 specular, half fresnelTerm, half grazingTerm) { half surfaceReduction = 1.0 / (roughness * roughness + 1.0); return half3(surfaceReduction * lerp(specular, grazingTerm, fresnelTerm)); } 合并效果\n最终结果 = 直接光漫反射 + 直接光镜面反射 + 间接光漫反射 + 间接光镜面反射\n/ Indirect BRDF // Indirect diffuse half3 indirectDiffuse = SH_indirectDiffuse(normalWS, AO); half3 indirectColor = indirectDiffuse * albedo; // Indirect Specualr half NoV = saturate(dot(normalWS, viewDirWS)); half fresnelTerm = Pow4(1.0 - NoV); half oneMinusReflectivity = OneMinusReflectivityMetallic(metalness); half reflectivity = half(1.0) - oneMinusReflectivity; half grazingTerm = saturate(reflectivity + smoothness); half3 indirectSpecular = IndirectSpecular(reflectVector, roughness, AO); half3 brdfSpecular = Calculate_F0(albedo, metalness); indirectColor += AO * indirectSpecular * EnvironmentBRDFSpecular(roughness, brdfSpecular, fresnelTerm, grazingTerm); 最后添加脸部和手部的emission fake SSS效果，最终合成效果如下\n参考 URP管线的自学HLSL之路 第三十七篇 造一个PBR的轮子 - 哔哩哔哩 (bilibili.com) Unity Shader - 搬砖日志 - URP PBR (抄作业篇，持续更新~)_Jave.Lin的博客-CSDN博客_unity urp的pbrshader [理论 - LearnOpenGL CN (learnopengl-cn.github.io)](https://learnopengl-cn.github.io/07 PBR/01 Theory/) URP管线PBR源码剖析（上） - 知乎 (zhihu.com) ","tags":["PBR shading","Unity","HLSL"],"title":"尝试在URP下手写 PBR shader"},{"categories":["技术实现"],"date":"February 5, 2023","permalink":"https://kampter.github.io/blog/urp%E4%BB%BF%E5%8E%9F%E7%A5%9E%E6%B8%B2%E6%9F%93shader/","section":"blog","summary":"技术实现 在原神二测就有来体验过游戏，也是一步一步看着原神成长壮大封神的，一直以来都对于这样认真做内容的游戏公司怀有敬意。这次终于“敢”在掌握一定技术的情况下来模仿原神的渲染效果。\n模型准备 模型下载自米哈游官方在B站开展的二创活动pmx格式，直接网上随便找个converter即可转为fbx方便使用，这里选择刻晴作为测试人物模型。而神奇的google有人通过神奇的方式获得其他的贴图文件。汇总起来一共获得这些文件：\n贴图分析 Lightmap.r = Specular Mask\nLightmap.g = AO Map\nLightmap.b = Roughness\nlightmap.a = Emission Mask\ndiffuse.a 在脸部，肌肉和上衣打底部分是由不为1的数值的\n两张Ramp贴图让我想到军团要塞2中的wrapdiffuse处理，思路应该类似。而大佬们的文章表示贴图里有十条Ramp，分为上下各五条 分别对应白天和晚上。\n贴图和基础计算准备 // Direction Function half3 viewDirWS = GetWorldSpaceViewDir(IN.positionWS); half3 normalWS = IN.normalWS; // Light Calculation Light mainLight = GetMainLight(); half4 lightColor = half4(mainLight.color, 1); //获取主光源颜色 half3 lightDir = normalize(mainLight.direction); //主光源方向 // Albedo half4 albedo = SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, IN.uv); half4 vertexColor = IN.vertexColor; //顶点色 // Texture Sampling // lightmap // R channel = Specular Mask // G channel = AO Map // B channel = Roughness // A channel = Emission Mask // 灰度1.0 ： 皮肤质感/头发质感（头发的部分是没有皮肤的） // 灰度0.7 ： 丝绸/丝袜 // 灰度0.5 ： 金属/金属投影 // 灰度0.3 ： 软的物体 // 灰度0.0 ： 硬的物体 half4 lightMap = SAMPLE_TEXTURE2D(_LightMap, sampler_LightMap, IN.uv); // Dot Product Function half ndotL = max(0, dot(normalWS, lightDir)); half ndotH = max(0, dot(normalWS, normalize(viewDirWS + lightDir))); half ndotV = max(0, dot(normalWS, viewDirWS)); half ndotU = max(0, dot(normalWS, (0, 1.0, 0))); Adjusted HalfLambert 基础的半兰伯特模型，加上用定点色做调整。\n//lambert half lambert = ndotL; half lambertAO = lambert * saturate(lightMap.g * 2); half lambertRampAO = smoothstep(0, _ShadowSmooth, lambertAO); // using vertex color as an offset to adjust half lambert sampling half halfLambertAO = saturate(lambertRampAO * _WrapDiffuse + 1 - _WrapDiffuse); half vertexOffset = step(0.5, vertexColor) == 1 ? vertexColor : 1 - vertexColor; half adjustedHalfSampler = saturate(halfLambertAO * vertexOffset); 这里 AO的地方自定义（lightmap.g ）乘2用来获得对比度更加明显的AO效果_ShadowSmooth 用来调整亮部到暗部过度的明显程度，左边0.5，右侧1.0。\nWrapped Diffuse 回顾一下军团要塞2中Ramp使用方式\nhalf4 ramp = SAMPLE_TEXTURE2D(_RampMap, sampler_RampMap, half2(halfLambert, 0)); 原神的衣服整体并不受环境光颜色直接影响，这里面需要计算是否是白天或者夜晚。\n// Day (0.5) or Night(0) half dayOrNight = (1 - step(0.1, _IsNight)) * 0.5 + 0.03; // diffuse = ramp + adjusted half lambert + AO half rampV = saturate(lightMap.a * 0.45 + dayOrNight); half2 rampUV = half2(adjustedHalfSampler, rampV); 把ramp的V方向取值，0-0.5为 黑天，0.5-1为白天，取代军团要塞中V方向只有单条ramp取值。\nSpecular 金属部分使用到metalMap高光，实现原理来自MatCap采样。\n// Metal Specular half metalMask = step(0.95, lightMap.r); half3 cameraForward = -viewDirWS; half3 viewUpDir = mul(UNITY_MATRIX_I_V, half4(0, 1, 0, 0)).xyz; half3 cameraRight = SafeNormalize(cross(viewUpDir, cameraForward)); half3 cameraUp = SafeNormalize(cross(cameraForward, cameraRight)); half2 metalMapUV = mul(half3x3(cameraRight, cameraUp, cameraForward), normalWS).xy * 0.49 + 0.5; half4 metalMap = SAMPLE_TEXTURE2D(_MetalMap, sampler_MetalMap, metalMapUV); half4 metalSpecular = _MetalIntensity * metalMap * metalMask * albedo; 非金属部分使用常规的Blinn Phong高光\nhalf Ks = 0.96; half SpecularPow = exp2(lightMap.r * 11.0 + 2.0); half SpecularNorm = (SpecularPow + 8.0) / 8.0; half4 SpecularColor = albedo * lightMap.g; half SpecularContrib = albedo * (SpecularNorm * pow(ndotH, SpecularPow)); half4 nonMetalSpecular = SpecularColor * SpecularContrib * ndotL* Ks * lightMap.b; 自发光\n这里我用自发光来描绘裸漏皮肤以及丝袜部分SSS材质的效果\n// Emission (Fake SSS) half emissionMask = albedo.a; emissionMask *= step(0.65, lightMap.a); // half time = abs((frac(_Time.y * 0.5) - 0.5) * 2); half4 emission = albedo * lightColor * emissionMask * _EmissionIntensity * _EmissionColor; Outline 多开一个pass，平滑法线后back face沿着法线方向平移，都是基础做法。其实也可以用后处理来做。但是URP 2022才支持不用写后处理脚本，直接贴matertial上去，后面在尝试做一版吧。\nPass { Name \u0026#34;OutLine\u0026#34; Tags {\u0026#34;LightMode\u0026#34; = \u0026#34;SRPDefaultUnlit\u0026#34;} Cull front HLSLPROGRAM #pragma vertex vert #pragma fragment frag Varyings vert(Attributes IN) { half4 scaledScreenParams = GetScaledScreenParams(); half ScaleX = abs(scaledScreenParams.x / scaledScreenParams.y); Varyings OUT; VertexPositionInputs vertexInput = GetVertexPositionInputs(IN.positionOS.xyz); VertexNormalInputs normalInput = GetVertexNormalInputs(IN.normal); half3 normalCS = TransformWorldToHClipDir(normalInput.normalWS); half2 extendDis = normalize(normalCS.xy) * (0.1 * 0.01); extendDis.x /=ScaleX ; OUT.positionCS = vertexInput.positionCS; OUT.positionCS.xy +=extendDis; return OUT; } half4 frag(Varyings IN) : SV_Target { return float4(0, 0, 0, 1); } ENDHLSL }\t暂时效果 还缺少深度空间边缘光，哦对头发的高光也应该额外处理，留个坑以后去实现。\n参考 【01】Unity URP 卡通渲染 原神角色渲染记录-Diffuse: Ramp + AO + Double Shadow - 知乎 (zhihu.com) 【Unity技术美术】 原神Shader渲染还原解析 - 知乎 (zhihu.com) [卡通渲染]二、原神角色渲染还原 - Diffuse-1 - 知乎 (zhihu.com) 原神角色渲染Shader分析还原 - 知乎 (zhihu.com) 【02】从零开始的卡通渲染-着色篇1 - 知乎 (zhihu.com) [卡通渲染]等宽屏幕空间边缘光 - 知乎 (zhihu.com) ","tags":["Toon shading","Unity","HLSL"],"title":"URP仿原神渲染shader"},{"categories":["课程笔记"],"date":"January 21, 2023","permalink":"https://kampter.github.io/blog/games-202-%E7%8E%AF%E5%A2%83%E5%85%89%E7%85%A7/","section":"blog","summary":"IBL实时环境光照 IBL：Image-Based Lighting\n典型的保存方式：cube map，spherical map\n在不考虑阴影的情况下（Visibility term）真实的渲染方程\n真实求解需要用蒙特卡洛积分求解path tracing, 但是速度太慢\n使用之前的近似方案\n一点点小的区别，我们只需要对 BRDF 覆盖的范围 ΩG 进行积分即可 第一部分的积分 红色区域就是对光源的入射方向（上面的 r ）进行了一个滤波\nprefilter，在 rendering 之前预先处理\n类似于 MIPMAP 的思想 预先生成多张使用不同滤波核 filter 的环境贴图 之后在 shading 的时候进行一个查询，双线性插值 如果查询的值不是一个预先设置的滤波核的大小，三线性插值 第二部分的积分 蓝色部分的积分：预先计算 precompute\n假定是 microfacet 的 BRDF\n只需要知道菲涅尔项、微表面的法线分布（roughness） Precompute its value for all possible combinations of variables roughness, color (Fresnel term), etc. 还是很难求积分 而且保存结果需要很大的内存 菲涅尔项可以用一个函数近似\nSchlick’s approximation 认为不同的材质是两个参数的函数：入射光夹角、基础反射率（基础颜色） D 项可以定义一个法线分布\nBeckmann distribution α 定义 roughness，分布的胖瘦 θh 表示法线和半角矢量的夹角 这样就可以简化为一个三维的预计算，但是计算量依旧很大\nUnreal Engine的降维思路 显式把上面的菲涅尔项写进去，试图把 R0 分离开来\n分母的 F 会被消掉 完整解释和求导公式 这样的好处是降维成二维纹理预计算 球面谐波函数 回顾Games101的知识点，diffuse一般保留低频信息 Spherical Harmonics\n球面谐波函数的可视化\n每一行的频率是一样的，第 l 阶的 SH m=2l+1 前 n 阶一共有 n2 个基函数 某天某个大佬想到用这个SH函数来描绘环境光照产生的diffuse信息。发现只需要使用前三阶的SH函数就可以把误差缩小到1%\n而SH可以支持旋转的性质解决了旋转问题：\n同一行的SH可以从同一行其他的SH旋转表示出来\nPrecomputed Radiance Transfer 利用球谐函数的性质进行预计算 用另一种角度看待渲染方程 我们假设场景是不变的，改变的只是光照 把 Lighting 表示为SH函数, 预计算Light transport部分 公式推演 将 diffuse 常数项从渲染方程中提取出来\n光照使用 SH 表示\n渲染方程变为\n红色部分与光照无关，可以预计算。渲染方程变为\n场景是不能动的，因为动了，visibility 项就变了，预计算失效 可以利用SH 的旋转性解决光源移动：如果光源做了一个旋转操作，很快就能够得到新的 SH 搜SH文章的时候看到这篇，对于这个观点有点感兴趣：声音是2D的信号，图像是3D的信号\n总结 无论 IBL 还是 PRT 都属于实现环境光照的方案，它们的区别在于：\nIBL 是一种从预计算环境光照出发的环境关照渲染方案： 采用环境贴图：存储占用空间较大，同时也占采样 I/O。 能保留高频信息，常用于 diffuse/glossy/specular 物体的渲染。 PRT 是一种从预计算 transfer function 出发的环境光照渲染方案： 采用 SH lighting：存储开销和重建环境光照的开销极低。 只能保留低频信息，常用于 diffuse/glossy 物体的渲染。 物体不可局部形变，材质不可动态：若发生变化，那么其 transfer 就需要更新。 只考虑了物体局部 transfer 效果，没有考虑完整场景 transfer 效果，不过在其它 PRT 方案中有支持完整场景的 transfer 效果。 参考 https://learnopengl-cn.github.io/07%20PBR/03%20IBL/02%20Specular%20IBL/\nhttps://www.cnblogs.com/KillerAery/p/15335369.html\nhttps://www.zhihu.com/column/c_1046337272613527552\n","tags":["Computer Graphic","Games 202","Math"],"title":"Games 202 环境光照"},{"categories":["技术实现"],"date":"January 16, 2023","permalink":"https://kampter.github.io/blog/%E5%86%9B%E5%9B%A2%E8%A6%81%E5%A1%9E2%E6%B8%B2%E6%9F%93shader%E5%AE%9E%E7%8E%B0/","section":"blog","summary":"技术分析 论文中有一篇非常形象的图片来实现渲染效果\nAlbedo // Albedo half4 mainTex = SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, IN.uv) * _MainColor; half4 albedo = mainTex; half4 ambient = half4(unity_SHAr.w, unity_SHAg.w, unity_SHAb.w, 1.0); 直接采样人物的贴图，顺手再把其他的贴图也一并采样进来。\nWraped diffuse 论文中的光照实现公式为\n在unity实现中使用ramp的方式即采样ramp时的uv 参数用halfLambert 组成的X分量，叠加上主光源即可。\n// Half Lambert half ndotL = max(0, dot(normalWS, lightDir)); half lambert = ndotL; half halfLambert= saturate(lambert * 0.5 + 0.5); half4 ramp = SAMPLE_TEXTURE2D(_RampMap, sampler_RampMap, half2(halfLambert, 0)); half4 wrapDiffuseTerm = ramp * lightColor; Ambient ambient实现比较复杂，这里直接抄unity urp中关于SH 求谐函数的代码\nreal3 SH_indirectDiffuse(real3 normalWS, real AO) { real4 SHCoefficients[7]; SHCoefficients[0] = unity_SHAr; SHCoefficients[1] = unity_SHAg; SHCoefficients[2] = unity_SHAb; SHCoefficients[3] = unity_SHBr; SHCoefficients[4] = unity_SHBg; SHCoefficients[5] = unity_SHBb; SHCoefficients[6] = unity_SHC; real3 color = SampleSH9(SHCoefficients, normalWS); return max(0, color) * AO; } View Independent Lighting Terms half3 viewIndependentLightTerms = albedo * (indirectDiffuse+ wrapDiffuseTerm); View Dependent Lighting Terms 论文中计算依赖视角的光照部分的公式\n在依赖视角的光照部分中，《军团要塞2》除了考虑一般的Phong高光外，还基于菲涅尔现象实现了类似边缘光的效果。Specular就是式子中的左半部分，Rim lighting就是式子中的右半部分。\n原版实现代码如下\n//计算View Independent Lighting Terms half3 viewIndependentLightTerms = albedo * (indirectDiffuse+ wrapDiffuseTerm); // Specular half fs = _FresnelSpecular; half4 spec = fs * pow(ndotH, smoothness); half4 specular = pow(spec, _KSpecular)* _SpecularColor * metalMask; // Rim Lightingw half kr = 0.5; half fr = pow(1 - ndotV, 4); half4 rim = kr * fr * pow(spec, _RimPower); // Dedicated Rim Lighting half4 aV = half(1); half4 dedicatedRimLighting = ndotU * fr * kr * aV; // Multiple Phong Terms half ks = 1; half4 multiplePhongTerms = ks * lightColor * max(specular, rim); half4 viewDependentLightTerms = multiplePhongTerms + dedicatedRimLighting; 参考没有遮罩的时候，转换成另一种算法，同时替代phong高光为blinn phong。\n添加Fresnel 效果的 multiplePhongTerms 高光模型\n// 计算View dependent Lighting Terms half halfVector = normalize(viewDirWS + lightDir); half3 specular = pow(ndotH, 1); half fresnel = pow(1 - dot(viewDirWS, halfVector), 5.0); fresnel += 0.5 * (1.0 - fresnel); half3 multiplePhongTerms = specular * fresnel * lightColor; Rim Light 实现 half rim = 1.0 - saturate(ndotV); half3 dedicatedRimLighting = _RimLightColor * pow(rim, _RimPower); 合成效果 half3 viewDependentLightTerms = albedo * lightColor * max(multiplePhongTerms, dedicatedRimLighting); // Combine half3 color = viewIndependentLightTerms + viewDependentLightTerms; 哦豁打完收工，忽略没有写投影pass。wrapped diffuse的效果提供明暗交界处有明显的泛红。\n参考 NPAR07_IllustrativeRenderingInTeamFortress2.pdf (akamaihd.net) 《军团要塞2》卡通渲染算法实现 - Richbabe的博客 | Richbabe Blog 【Shader拓展】Illustrative Rendering in Team Fortress 2_妈妈说女孩子要自立自强的博客-CSDN博客 Shader - Specular Term - Fresnel — polycount ","tags":["Toon Shading","Unity","HLSL"],"title":"军团要塞2渲染shader实现"},{"categories":["课程笔记"],"date":"January 11, 2023","permalink":"https://kampter.github.io/blog/games-202-%E9%98%B4%E5%BD%B1/","section":"blog","summary":"ShadowMapping 原理 先渲染一个从光源到物体的pass 获得深度图 再渲染一个从相机位置到物体的pass获得深度图，并且把这个深度投影到光源位置 （图中橙色点为无阴影） 比较第一次与第二次投影到光源的深度，如果相同即没有阴影；如果比原深度远，即在阴影中(图中红色点为在阴影中) 缺陷 走样、分辨率。数值精度问题 只能点光源、硬阴影 会产生自遮挡如果light 方向与物体平面接近平行 改善方式具体方式就是当一个点深度大于记录深度的值超过一个阈值bias时，我们才认为这个点在阴影内。 解决方案 增加一个 bias\n中间的黄色那段我们不算\n也就是说我们对计算得到的深度减去一个 bias 一些技巧：动态的 bias\n当光线和物体表面法线夹角比较大时，bias 也需要比较大 当光线和物体表面法线夹角比较小时，bias 比较小即可 不合适的bias会出现阴影与物体中间出现间隔\n\u0026mdash; 2022.12.15 更新 \u0026mdash;\nShadow map已经又引擎自动计算好不需要手动实现。\nURP中对shadow map 采样。\n其中开启阴影投射和接收的关键字为:\n#pragma multi_compile _ _MAIN_LIGHT_SHADOWS #pragma multi_compile _ _MAIN_LIGHT_SHADOWS_CASCADE #pragma multi_compile _ _SHADOWS_SOFT 在计算Light的时候，遍历每个光源的light.shadowattenuation加算到diffuse color中就可以计算出阴影颜色。而完整的阴影投射在URP中需要单独在shadowCaster这个pass中计算，同时调用ApplyShadowBia()来计算上文中提到的bias解决方案。\nShader \u0026#34;Custom/ShadowReciver\u0026#34; { Properties { _BaseColor (\u0026#34;Base Color\u0026#34;, Color) = (1.0, 1.0, 1.0, 1.0) _BaseMap (\u0026#34;Main Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34;{} _SpecColor (\u0026#34;Specular Color\u0026#34;, Color) = (1.0, 1.0, 1.0, 1.0) _Smoothness (\u0026#34;Gloss\u0026#34;, range(8, 256)) = 20 _BumpMap (\u0026#34;Normal Map\u0026#34;, 2D) = \u0026#34;bump\u0026#34; {} _BumpScale (\u0026#34;Scale\u0026#34;, Float) = 1.0 [Toggle(_MULTIPLE_LIGHTS)] _MultipleLights (\u0026#34;Received MultipleLights\u0026#34;, Float) = 1.0 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;RenderPipeline\u0026#34;=\u0026#34;UniversalRenderPipeline\u0026#34; \u0026#34;LightMode\u0026#34; = \u0026#34;UniversalForward\u0026#34; \u0026#34;ShaderModel\u0026#34;=\u0026#34;4.5\u0026#34; } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; CBUFFER_START(UnityPerMaterial) half4 _BaseMap_ST; half4 _BumpMap_ST; half4 _BaseColor; half4 _SpecColor; half _Smoothness; half _BumpScale; CBUFFER_END TEXTURE2D(_BaseMap); SAMPLER(sampler_BaseMap); TEXTURE2D(_BumpMap); SAMPLER(sampler_BumpMap); struct Attributes { half4 positionOS : POSITION; half3 normal : NORMAL; half4 tangetOS : TANGENT; half2 uv : TEXCOORD0; }; struct Varyings { half4 positionCS : SV_POSITION; half3 positionWS : POSITION_WS; half4 positionSC : POSITION_SC; half2 uv : TEXCOORD0; half3 normalWS : NORMAL_WS; half4 tangentWS : TANGENT_WS; }; half4 LightingModelImplement (Light light, half3 normalWS, half3 viewDirWS, half2 uv, bool isMainLight) { half3 lightColor = light.color; half3 lightDir = normalize(light.direction); half lambert = saturate(dot(lightDir, normalWS)); half3 albedo = SAMPLE_TEXTURE2D(_BaseMap, sampler_BaseMap, uv) * _BaseColor; //half3 ambient = half3(unity_SHAr.w, unity_SHAg.w, unity_SHAb.w) * albedo; half3 ambient = SampleSH(normalWS) * albedo; half3 diffuse = lambert * lightColor * ambient; half3 halfDir = normalize(viewDirWS + lightDir); half3 specular = pow(saturate(dot(normalWS, halfDir)), _Smoothness) * lightColor * saturate(_SpecColor); half4 finalColor = half4(ambient + diffuse + specular, 1.0) * light.shadowAttenuation * light.distanceAttenuation; return finalColor; } ENDHLSL Pass { Tags{ \u0026#34;LightMode\u0026#34;=\u0026#34;UniversalForward\u0026#34; } HLSLPROGRAM #pragma vertex vert #pragma fragment frag #pragma shader_feature _MULTIPLE_LIGHTS #pragma multi_compile _ _MAIN_LIGHT_SHADOWS #pragma multi_compile _ _MAIN_LIGHT_SHADOWS_CASCADE #pragma multi_compile _ _SHADOWS_SOFT #pragma multi_compile _ _ADDITIONAL_LIGHTS_VERTEX _ADDITIONAL_LIGHTS Varyings vert (Attributes IN) { const VertexPositionInputs vertex_position_inputs = GetVertexPositionInputs(IN.positionOS); const VertexNormalInputs vertex_normal_inputs = GetVertexNormalInputs(IN.normal); // get positive or negative normal signal (should be either 1 or -1) half sign = IN.tangetOS.w * GetOddNegativeScale(); Varyings OUT; OUT.positionWS = vertex_position_inputs.positionWS; OUT.positionCS = vertex_position_inputs.positionCS; OUT.normalWS = vertex_normal_inputs.normalWS; OUT.positionSC = GetShadowCoord(vertex_position_inputs); OUT.uv = TRANSFORM_TEX(IN.uv, _BaseMap); OUT.tangentWS = half4(vertex_normal_inputs.tangentWS, sign); return OUT; } half4 frag (Varyings IN) : SV_Target { half3 normalTS = UnpackNormalScale(SAMPLE_TEXTURE2D(_BumpMap, sampler_BumpMap, IN.uv), _BumpScale); half3 biTangent = IN.tangentWS.w * cross(IN.normalWS, IN.tangentWS.xyz); half3 normalWS = mul(normalTS, half3x3(IN.tangentWS.xyz, biTangent, IN.normalWS)); half3 viewDirWS = SafeNormalize((GetCameraPositionWS() - IN.positionWS)); Light mainLight = GetMainLight(IN.positionSC); half4 finalColor = LightingModelImplement(mainLight, normalWS, viewDirWS, IN.uv, true); #if _MULTIPLE_LIGHTS int lightsCount = GetAdditionalLightsCount(); for (int i=0; i\u0026lt;lightsCount; i++) { Light light = GetAdditionalLight(i, IN.positionWS, half4(1, 1, 1, 1)); finalColor += LightingModelImplement(light, normalWS, viewDirWS, IN.uv, false); half shadow = AdditionalLightRealtimeShadow(i, IN.positionWS, mainLight.direction); finalColor *= shadow; } #endif return finalColor; } ENDHLSL } Pass{ Tags{ \u0026#34;LightMode\u0026#34;=\u0026#34;ShadowCaster\u0026#34; } ZWrite On // the only goal of this pass is to write depth! ZTest LEqual // early exit at Early-Z stage if possible ColorMask 0 // we don\u0026#39;t care about color, we just want to write depth, ColorMask 0 will save some write bandwidth Cull Back HLSLPROGRAM #pragma vertex vertShadow #pragma fragment fragShadow half3 _LightDirection; Varyings vertShadow(Attributes IN) { const VertexPositionInputs vertex_position_inputs = GetVertexPositionInputs(IN.positionOS); const VertexNormalInputs vertex_normal_inputs = GetVertexNormalInputs(IN.normal); Varyings OUT; OUT.positionWS = vertex_position_inputs.positionWS; OUT.normalWS = vertex_normal_inputs.normalWS; OUT.positionCS = TransformWorldToHClip(ApplyShadowBias(OUT.positionWS,OUT.normalWS, _LightDirection)); #if UNITY_REVERSED_Z OUT.positionCS.z = min(OUT.positionCS.z, UNITY_NEAR_CLIP_VALUE); #else OUT.positionCS.z = max(OUT.positionCS.z, UNITY_NEAR_CLIP_VALUE); #endif OUT.uv = IN.uv; return OUT; } half4 fragShadow(Varyings IN):SV_TARGET { return 0; } ENDHLSL } } } 多光源阴影还有些问题，后面需要修一下bug。\n数学知识 某年某月某日的某个变态搞了个数学公式，用不等式来代替等式。 $$ \\int_{\\Omega} f(x) g(x) \\mathrm{d} x \\approx \\frac{\\int_{\\Omega} f(x) \\mathrm{d} x}{\\int_{\\Omega} \\mathrm{d} x} \\cdot \\int_{\\Omega} g(x) \\mathrm{d} x $$ 使用例子：Rendering Equation $$ L_{o}\\left(p, \\omega_{o}\\right)=L_{e}\\left(p, \\omega_{o}\\right)+\\int_{\\Omega^{+}} L_{i}\\left(p, \\omega_{i}\\right) f_{r}\\left(p, \\omega_{i}, \\omega_{o}\\right) cos(\\theta_i) V(p,\\omega_{i}) \\mathrm{d} \\omega_{i} $$ 把Visibility拆出来： $$ L_{o}\\left(\\mathrm{p}, \\omega_{o}\\right) \\approx \\frac{\\int_{\\Omega^{+}} V\\left(\\mathrm{p}, \\omega_{i}\\right) \\mathrm{d} \\omega_{i}}{\\int_{\\Omega^{+}} \\mathrm{d} \\omega_{i}} \\cdot \\int_{\\Omega^{+}} L_{i}\\left(\\mathrm{p}, \\omega_{i}\\right) f_{r}\\left(\\mathrm{p}, \\omega_{i}, \\omega_{o}\\right) \\cos \\theta_{i} \\mathrm{~d} \\omega_{i} $$ 因此其表示的意义就是,我们计算每个点的shading，然后去乘这个点的visibality得到的就是最后的渲染结果。\nPCF 软阴影 假设这是我们在shadow map中获取到的深度值，而P点得到的实际到光源深度为0.5，这时所有的在shadow map中获取到的深度值要与P点得到的实际到光源深度，即0.5进行比较，所有大于0.5的像素我们输出0，反之则输出\nPCSS 软阴影 PCF 的思想，动态调整核的大小\n什么地方需要硬阴影，什么地方需要硬阴影？\n遮挡物和阴影的距离\n距离越大，阴影越软 距离越小，阴影越硬 根据相似三角形\npenumbra：半影 参考 https://zhuanlan.zhihu.com/p/523775521 https://banbao991.github.io/2021/06/18/CG/Algorithm/SM-PCF-PCSS-VSM/ https://www.cnblogs.com/KillerAery/p/15201310.html ","tags":["Computer Graphic","Math","HLSL","Unity"],"title":"Games 202 - 阴影"},{"categories":["Portfolio"],"date":"December 25, 2022","permalink":"https://kampter.github.io/blog/%E7%A8%8B%E5%BA%8F%E7%9B%B8%E5%85%B3/","section":"blog","summary":"Unity 仿只狼抓钩系统 Github\nBlender 插件 自定义转盘 拼写检查 实现原理\n","tags":["unity","blender"],"title":"程序相关"},{"categories":["Portfolio"],"date":"December 25, 2022","permalink":"https://kampter.github.io/blog/%E6%B8%B2%E6%9F%93%E7%9B%B8%E5%85%B3/","section":"blog","summary":"URP 后处理描边 实现原理\nURP 后处理雾效 实现原理\nDisney Principled BRDF 实现 实现原理\nGlobal Game Jam 2023 （包括草地，树木，水体渲染） 实现原理\nURP 仿原神渲染 实现原理\n仿军团要塞2渲染 实现原理\n","tags":["unity","hlsl"],"title":"渲染相关"},{"categories":["Portfolio"],"date":"December 25, 2022","permalink":"https://kampter.github.io/blog/%E6%B8%B8%E6%88%8F%E7%89%B9%E6%95%88/","section":"blog","summary":"Poison Projectile Trail by UE 实现原理\nLighting Impact by Unity 实现原理\nBubble Effect by Unity 实现原理\nLighting Swipe Trail by Unity Stylized Projectile by Unity {{youtube s9sVoHDX0EM}}\nPortal Effect by Unity Glitch by Unity Water Ripples by Unity ","tags":["unity","unreal engine","VFX Graph","niagara system","substance designer"],"title":"游戏特效"},{"categories":["课程笔记"],"date":"September 28, 2022","permalink":"https://kampter.github.io/blog/games-101-pbr%E5%8E%9F%E7%90%86/","section":"blog","summary":"前言：只记录自己需要的内容\n辐射度量学 Radiant energy 能量，单位：J（Joule焦耳）\nRadiant Flux （单位时间能量 → 功率）单位：W（Watt），lm（lumen流明）\nIntensity the power per unit solid angle (立体角) $$ I(\\omega) = \\frac{\\mathrm{d} \\Phi}{\\mathrm{d} \\omega} $$\n立体角\n面积/半径^2， $$ \\Omega=\\frac{A}{r^{2}} $$\n单位：sr, steradians $$ \\mathrm{d} \\omega=\\frac{\\mathrm{d} A}{r^{2}}=\\sin \\theta \\mathrm{d} \\theta \\mathrm{d} \\phi $$\n方向性：用 w来表示方向\nIrradiance power per unit area $$ E(\\mathbf{x}) \\equiv \\frac{\\mathrm{d} \\Phi(\\mathbf{x})}{\\mathrm{d} A} $$ 单位：W/m^2 | lm/m^2 = lux\nirradiance解释衰减, 随着半径增大衰减越明显\nRadiance power per unit solid angle, per projected unit area. $$ L(\\mathrm{p}, \\omega) \\equiv \\frac{\\mathrm{d}^{2} \\Phi(\\mathrm{p}, \\omega)}{\\mathrm{d} \\omega \\mathrm{d} A \\cos \\theta} $$ 非垂直平面需要根据 θ 夹角计算\nradiance 与intensity 和 irradiance 结合对比\nIrradiance和Radiance之间的区别就在于是否有方向性\nIrradiance: power received by area dA 四面八方的光线积分起来 Radiance: power received by area dA from “direction” dω $$ \\begin{aligned} d E(\\mathrm{p}, \\omega) \u0026amp;=L_{i}(\\mathrm{p}, \\omega) \\cos \\theta \\mathrm{d} \\omega \\ E(\\mathrm{p}) \u0026amp;=\\int_{H^{2}} L_{i}(\\mathrm{p}, \\omega) \\cos \\theta \\mathrm{d} \\omega \\end{aligned} $$\nBidirectional Reflectance Distribution Function (BRDF) 双向反射分布函数 理解 反射的理解：光线打到某个点，（被吸收了）然后反弹（发出）到其他地方\n某个点从某个方向接受/向某个方向发射光线能量：radiance\nDifferential irradiance incoming: $$ d E\\left(\\omega_{i}\\right)=L\\left(\\omega_{i}\\right) \\cos \\theta_{i} d \\omega_{i} $$\nDifferential radiance exiting: $$ d L_{r}\\left(\\omega_{r}\\right) $$\n通俗理解：这个微小面积接受的能量计算后，再分配到各个方向上\nThe Reflection Equation 反射方程 针对一个输出源（着色点），积分所有方向输入源（光照）的BRDF，获得最后的输出 $$ L_{r}\\left(\\mathrm{p}, \\omega_{r}\\right)=\\int_{H^{2}} f_{r}\\left(\\mathrm{p}, \\omega_{i} \\rightarrow \\omega_{r}\\right) L_{i}\\left(\\mathrm{p}, \\omega_{i}\\right) \\cos \\theta_{i} \\mathrm{d} \\omega_{i} $$\nThe Rendering Equation 渲染方程 $$ L_{o}\\left(p, \\omega_{o}\\right)=L_{e}\\left(p, \\omega_{o}\\right)+\\int_{\\Omega^{+}} L_{i}\\left(p, \\omega_{i}\\right) f_{r}\\left(p, \\omega_{i}, \\omega_{o}\\right)\\left(n \\cdot \\omega_{i}\\right) \\mathrm{d} \\omega_{i} $$\n渲染方程考虑多个点光源：根据w方向加起来，同时考虑其他物体反射的光源。\n一个神奇的抽象化理解：\n光栅化的着色过程主要是直接光照（间接光照计算非常复杂） 全局光照：直接和间接光照的集合 会收敛到一个亮度 BRDF 材质 以前只有Blinn Phong的时候，通过非物理的方式模拟出各种材质。\nMaterial == BRDF 决定光如何被反射\nDiffuse / Lambertain Material 能量守恒：进出的irradiance相同（总量） 漫反射：出的radiance均匀→ $f_r = c$ （常量） 假设入射光和出射光都是均匀的→ $L_i = L_o$ ρ: albedo(反射率, RGB) Glossy Material（BRDF） Ideal reflective / refractive material (BSDF * ) BSDF（散射） = BRDF（反射） + BTDF（折射）\nIsotropic / Anisotropic Materials (BRDFs) 各向异性BRDF\nFresnel Reflection / Term （菲涅耳项） 和normal法线方向越接近，越少光被反射\n近似：Schlick’s approximation $$ \\begin{aligned} R(\\theta) \u0026amp;=R_{0}+\\left(1-R_{0}\\right)(1-\\cos \\theta)^{5} \\ R_{0} \u0026amp;=\\left(\\frac{n_{1}-n_{2}}{n_{1}+n_{2}}\\right)^{2} \\end{aligned} $$\nMicrofacet Material 微表面材质 从足够远的地方看过去，无限接近镜面反射。但是不断拉近，表面会变得凹凸不平。\n远处情况就是Glossy, 近处是diffuse，因此获得计算公式如下： $$ f(\\mathbf{i}, \\mathbf{o})=\\frac{\\mathbf{F}(\\mathbf{i}, \\mathbf{h}) \\mathbf{G}(\\mathbf{i}, \\mathbf{o}, \\mathbf{h}) \\mathbf{D}(\\mathbf{h})}{4(\\mathbf{n}, \\mathbf{i})(\\mathbf{n}, \\mathbf{o})} $$\nFresnel term 表示菲尼尔效果 shadowing-masking term 表示足够远的地方光滑但是细微的地方实际是凹凸不平产生遮挡阴影效果 distribution of normals 表示凹凸不平的法线方向（细小平面的方向） \u0026mdash; 2023.2.10 更新 \u0026mdash;\n尝试在URP中手写PBR shader\n","tags":["Computer Graphic","Math","Games 101"],"title":"Games 101 - PBR原理"},{"categories":["课程笔记"],"date":"September 11, 2022","permalink":"https://kampter.github.io/blog/games-101-shading/","section":"blog","summary":"前言：只记录自己需要的内容\n深度测试 （Z buffer or Depth buffer） 把深度视为无限远，然后遍历每一个山脚行，再遍历每一个三角形的光栅化过程，同时记录光栅化的深度信息。如果光栅化后当前像素点的深度信息小于之前记录过的信息，则替换原本的像素点信息。\nZ-buffer：对每个像素多存一个深度\n复杂度：O(n) for n triangles 并不是排序，而是只要最值\n需要保证三角形进入顺序和结果无关\n无法处理透明物体\nBlinn-Phong Reflectance Model 光照模型 着色模型 Diffuse Lambertian (Diffuse) Shading $$ L_{d}=k_{d}\\left(I / r^{2}\\right) \\max (0, \\mathbf{n} \\cdot \\mathbf{l}) $$\n可以简化为：\nK_d 漫反射系数\nI/r2 衰减后的灯光，LightColor * AttenuationLight\nMax(0, normal dot LightDirction)\nSpecular p一般100～200，控制高光大小\nAmbient $$ L_{a}=k_{a} I_{a} $$\n一般可以认为ambient为一个常数，要计算真实光照要考虑GI\nCombine $$ \\begin{aligned} L \u0026amp;=L_{a}+L_{d}+L_{s} \\ \u0026amp;=k_{a} I_{a}+k_{d}\\left(I / r^{2}\\right) \\max (0, \\mathbf{n} \\cdot \\mathbf{l})+k_{s}\\left(I / r^{2}\\right) \\max (0, \\mathbf{n} \\cdot \\mathbf{h})^{p} \\end{aligned} $$\n\u0026mdash; 2022.12.20 更新 \u0026mdash;\nURP实现\nShader \u0026#34;Custom/Blinn-Phong\u0026#34; { Properties { _BaseColor (\u0026#34;Base Color\u0026#34;, Color) = (1.0, 1.0, 1.0, 1.0) _BaseMap (\u0026#34;Main Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34;{} _SpecColor(\u0026#34;Specular Color\u0026#34;, Color) = (1.0, 1.0, 1.0, 1.0) _Smoothness(\u0026#34;Smoothness\u0026#34;, range(8, 256)) = 20 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;RenderPipeline\u0026#34;=\u0026#34;UniversalRenderPipeline\u0026#34; \u0026#34;LightMode\u0026#34; = \u0026#34;UniversalForward\u0026#34; \u0026#34;ShaderModel\u0026#34;=\u0026#34;4.5\u0026#34; } LOD 100 Pass { HLSLPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; struct Attributes { half4 positionOS : POSITION; half3 normal : NORMAL; half4 tangetOS : TANGENT; half2 uv : TEXCOORD0; }; struct Varyings { half4 positionCS : SV_POSITION; half3 positionWS : POSITION_WS; half2 uv : TEXCOORD0; half3 normalWS : NORMAL_WS; }; TEXTURE2D(_BaseMap); SAMPLER(sampler_BaseMap); CBUFFER_START(UnityPerMaterial) half4 _BaseMap_ST; half4 _BaseColor; half4 _SpecColor; half _Smoothness; CBUFFER_END Varyings vert (Attributes IN) { const VertexPositionInputs vertex_position_inputs = GetVertexPositionInputs(IN.positionOS); const VertexNormalInputs vertex_normal_inputs = GetVertexNormalInputs(IN.normal); Varyings OUT; OUT.positionWS = vertex_position_inputs.positionWS; OUT.positionCS = vertex_position_inputs.positionCS; OUT.uv = TRANSFORM_TEX(IN.uv, _BaseMap); OUT.normalWS = vertex_normal_inputs.normalWS; return OUT; } half4 frag (Varyings IN) : SV_Target { Light mainLight = GetMainLight(); half3 lightColor = mainLight.color; half3 lightDir = normalize(mainLight.direction); half3 lightAtten = mainLight.distanceAttenuation; // diffuse half3 albedo = SAMPLE_TEXTURE2D(_BaseMap, sampler_BaseMap, IN.uv) * _BaseColor; half kd = albedo; half light = lightColor * lightAtten; half ndotL = max(0, dot(IN.normalWS, lightDir)); half3 diffuse = kd * light * ndotL; // specular half ks = 1 - kd; half3 viewDirWS = normalize(GetCameraPositionWS() - IN.positionWS); half3 halfDir = normalize(viewDirWS + lightDir); half nDotH = max(0, dot(halfDir, IN.normalWS)); half3 specular = ks * light * pow(nDotH, _Smoothness) * saturate(_SpecColor); // Ambient half3 ambient = 0.1; // Combine half4 finalColor = half4(ambient + diffuse + specular, 1.0); return finalColor; } ENDHLSL } } } 效果如下\nReal Time Rendering 应用阶段：CPU传递数据给GPU（包括纹理，材质，着色器，是否Culling）。Drawcall 即CPU每调用一次GPU就计算一次Drawcall\n几何阶段：顶点着色器，几何着色器，曲面细分着色器，可编程。几何阶段还进行投影，裁切和屏幕映射等操作。\n几何着色器常见应用：法线可视化，绘制图形（增加删除点）。曲面细分着色器常见应用: displacement 置换\n投影：观察空间转换到裁剪空间（又被称为齐次裁剪空间）裁切：对透视裁剪空间来说，GPU需要对裁剪空间中的顶点执行齐次除法（其实就是将齐次坐标系中的w分量除x、y、z分量），得到顶点的归一化的设备坐标（Normalized Device Coordinates, NDC）屏幕映射：最终转化为屏幕空间，Z分量即Z-buffer\n光栅化阶段：包括图元组装（三个点组成一个三角形），三角形遍历（参考光栅化笔记），片元着色器，逐片元操作包括裁剪测试（Scissor Test）、透明度测试（Alpha Test）、模板测试（Stencil Test）以及深度测试（Depth Test），可自定义。\n通过测试的片元可以进入合并，考虑是否颜色混合 （alpha blend）在经过上面的层层测试后，片元颜色就会被送到颜色缓冲区。GPU会使用双重缓冲（Double Buffering）的策略，即屏幕上显示前置缓冲（Front Buffer），而渲染好的颜色先被送入后置缓冲（Back Buffer），再替换前置缓冲，以此避免在屏幕上显示正在光栅化的图元。\n纹理映射 纹理映射即漫反射系数k_d\n每一个3D模型的表面展开都是1个2d平面，2d平面的x y坐标用来查询纹理的uv坐标\n插值方式，V是三角形abc的重心 $$ \\begin{aligned} \\alpha \u0026amp;=\\frac{-\\left(x-x_{B}\\right)\\left(y_{C}-y_{B}\\right)+\\left(y-y_{B}\\right)\\left(x_{C}-x_{B}\\right)}{-\\left(x_{A}-x_{B}\\right)\\left(y_{C}-y_{B}\\right)+\\left(y_{A}-y_{B}\\right)\\left(x_{C}-x_{B}\\right)} \\ \\beta \u0026amp;=\\frac{-\\left(x-x_{C}\\right)\\left(y_{A}-y_{C}\\right)+\\left(y-y_{C}\\right)\\left(x_{A}-x_{C}\\right)}{-\\left(x_{B}-x_{C}\\right)\\left(y_{A}-y_{C}\\right)+\\left(y_{B}-y_{C}\\right)\\left(x_{A}-x_{C}\\right)} \\ \\gamma \u0026amp;=1-\\alpha-\\beta \\end{aligned} $$ 投影前后的重心坐标可能会变化，所以需要在对应时间计算对应的重心坐标来做插值，不能随意复用！\n纹素 每张纹理贴图的一个像素点。\n如果纹素太小，把多个pixel 映射同一个纹素\n解决： Nearest Bilinear Bilinear 插值 lerp 水平+竖直插值→双线性插值 最近的四个点插值 Bicubic 双向三次插值 周围16个点做三次插值 运算量更大，结果更好 如果纹素太大，会产生摩尔纹\n用Mipmap降低纹理精度\n各向异性过滤\n把贴图的xy方向分别使用不同的mipmap，比如1024x1024的贴图可以变成128 x 256或者64 x 128\n怎么知道层数D？约为相邻pixel的映射uv之间的距离取2的对数 如果计算出来需要的D是整数，就很方便，直接查找 如果计算出来需要的D不是整数→Trilinear Interpolation三线性插值 参考 https://zhuanlan.zhihu.com/p/137780634 ","tags":["Computer Graphic","Math","HLSL","Unity","Games 101"],"title":"Games 101 - Shading"},{"categories":["学习笔记"],"date":"September 2, 2022","permalink":"https://kampter.github.io/blog/games-101-%E5%85%89%E6%A0%85%E5%8C%96/","section":"blog","summary":"前言：只记录自己需要的内容\n光栅化 屏幕空间 从（0，0）到（width, height）\n光栅化的过程即采样过程，来判断屏幕上的像素点是否在三角形里面\n采样的缺点：以点代面，有失偏颇 → Aliasing 走样，表现为锯齿\n引入傅里叶变换，采样信息可以分解成函数表达\n高频信息：简单来说就是outline边界，边界变化带来巨大的图像效果变化\n只保留高频信息：表现为边缘部分\n只保留低频信息：表现为画面主体\n走样 走样就是采样的中出现重复或者没有采集到关键信息 （图中蓝色和黑色采样点相同，但是实际差距明显）\n减少Aliasing Error 的方法\n增加采样率 Antialiasing 先模糊（砍掉高频信号也就是画面边缘容易走样的位置）再采样 MSAA 超采样 （增加单像素点内的采样点） FXAA 图像后期处理找到边界部分（走样的部分）替换成不走样的图像 TAA 更立体的在相邻帧采样同一像素的不同位置 ","tags":["Computer Graphic","Math","Games 101"],"title":"Games 101 - 光栅化"},{"categories":["课程笔记"],"date":"August 21, 2022","permalink":"https://kampter.github.io/blog/games-101-%E7%BA%BF%E4%BB%A3%E5%92%8C%E5%8F%98%E6%8D%A2/","section":"blog","summary":"前言：只记录自己需要的内容\n线代 判断点是否在三角形内的方法1 按照逆时针找到ab，bc，ca向量，当同时满足cross(ab * ap) \u0026gt; 0 \u0026amp;\u0026amp; cross(bc * bp) \u0026gt; 0 \u0026amp;\u0026amp; cross (ca * cp) \u0026gt; 0 时点在三角形内，如果有一个结果为0 点在这条边上，有一个小于0在三角形外。换种思路就是如果点在三角形内测，三次判断点都在封闭线段的左侧，即点在图形内。\n判断点是否在三角形内的方法2 从远处发射一条光线，到无限远，如果与多边形交点为偶数，在图形外，否则在图形内。遍历多边形每一条边与光线相交，如果交点在这条边上，交点数 +1。\n变换 齐次空间: 空间变化增加一个维度实现Transformation （图例为二维变换，三位变化同理增加一个维度到四维） 这样带来的好处是在一个矩阵就能完成Transformation操作 （Affine map是原本Transformation 需要两个矩阵操作）\n模型空间：以模型为世界原点的坐标系（Local） Model transformation\n世界空间：以世界坐标系（0，0，0）为原点，将模型摆放在世界Global坐标系中 View transformation\n相机/观察空间：我们看到的画面由摄像机捕捉，摄像机参数决定了我们在屏幕上看到的东西，这一步可以将世界坐标系转换到摄像机坐标系。 Projection transformation\n正交投影和透视投影：这也是Zbuffer储存深度信息的由来。投影变化包括从view变化到正交投影再变化到透视投影才输出至viewport视口显示再屏幕上。\n投影的取值范围为[-1, 1]\n\u0026mdash; 2023.02.04 更新 \u0026mdash;\n额外帮助理解的图片：空间变化的过程\n以及在unity中实现空间变换部分的代码及说明\nHCS齐次裁剪空间的xy坐标范围在[-w, w]之间\n而NDC空间，是把视锥放映射到 -1 到 1的正方体中，也就是把HCS空间投影到[-1, 1]的范围内\n参考 【04】Unity URP 卡通渲染 原神角色渲染记录-Depth-Based Effect: 7Spaces + 屏幕空间等距深度边缘光Rim Light - 知乎 (zhihu.com) ","tags":["Computer Graphic","Math","Games 101"],"title":"Games 101 - 线代和变换"},{"categories":["日常生活"],"date":"December 21, 2019","permalink":"https://kampter.github.io/blog/%E7%A2%8E%E7%89%87%E5%8C%96%E8%AE%B0%E5%BD%95%E6%BE%B3%E6%B4%B2%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%94%9F%E6%B4%BB/","section":"blog","summary":"ANU - Canberra 学习java的时候尝试复刻flappy bird\n终于到了最喜欢的图形学课程。可以用Java 版本的opengl 绘画。\n也在老师的帮助指导下，完成了Lighting model以及简单光线追踪模型。\n我还专门选修了一门制作网页的课程。这是当时制作网站的效果，完全静态网站，没有使用任何的框架，纯html, cs, javascript实现的网站效果。那时候觉得手写响应式网站还要不断适配各种设备可太累了，痛恨老师不让我们用框架。\nVU - Melbourne 作为一个名商科学生偷学AE合成课程的作品，学会了在AE中抠图，添加素材，定位追踪，求三维空间位置，并且最终调色合成。\n这是那节课上课时候有机会参观影视实拍流程的绿幕。我的第一个AE合成作品便使用这里实现的把拍摄素材融入到视频中。\nEVE 悉尼见面会 在堪培拉枯燥的学习生活之余终于有机会去悉尼参加我喜欢的游戏eve的粉丝见面会。\n提前知道三神教船只信息，有点过分超模\n还有机会组队现场PVP，我太菜了，作为一个pve玩家被暴打。\n","tags":["Java","Opengl","Eve Online","Computer Graphic"],"title":"碎片化记录一下在澳洲的学习生活"},{"categories":["Portfolio","课程笔记"],"date":"February 17, 2018","permalink":"https://kampter.github.io/blog/maya-bifrost-showreel/","section":"blog","summary":" R\u0026amp;D \u0026ndash;\u0026gt; WIP \u0026ndash;\u0026gt; Render Test \u0026ndash;\u0026gt; Final Render\nBeach Beer Waterfall Coffee 第一周 第二周 第三周 第四周 第五周 第六周 第七周 第八周 第九周 第十周 第十一周 ","tags":["Maya","Bifrost","After Effect"],"title":"Maya Bifrost"},{"categories":["Portfolio"],"date":"December 18, 2017","permalink":"https://kampter.github.io/blog/houdini-showreel/","section":"blog","summary":"Particle Trails Water Cathedral Beauty ","tags":["Houdini","Flip","Particle"],"title":"Houdini"},{"categories":null,"date":"January 1, 0001","permalink":"https://kampter.github.io/portfolio/blender%E5%91%BD%E5%90%8D%E8%A7%84%E5%88%99%E6%A3%80%E6%9F%A5/","section":"portfolio","summary":"","tags":null,"title":"Blender命名规则检查"},{"categories":null,"date":"January 1, 0001","permalink":"https://kampter.github.io/portfolio/blender%E8%BD%AC%E7%9B%98%E8%8F%9C%E5%8D%95/","section":"portfolio","summary":"","tags":null,"title":"Blender转盘菜单"},{"categories":null,"date":"January 1, 0001","permalink":"https://kampter.github.io/portfolio/disney-pbr/","section":"portfolio","summary":"","tags":null,"title":"Disney PBR"},{"categories":null,"date":"January 1, 0001","permalink":"https://kampter.github.io/portfolio/%E4%BB%BF%E5%8E%9F%E7%A5%9E%E6%B8%B2%E6%9F%93/","section":"portfolio","summary":"","tags":null,"title":"Genshin Impact"},{"categories":null,"date":"January 1, 0001","permalink":"https://kampter.github.io/portfolio/ggj2023/","section":"portfolio","summary":"","tags":null,"title":"GGJ2023 x GiCA Groot项目"},{"categories":null,"date":"January 1, 0001","permalink":"https://kampter.github.io/portfolio/grappling-hock/","section":"portfolio","summary":"Video ","tags":null,"title":"Grappling Hock"},{"categories":null,"date":"January 1, 0001","permalink":"https://kampter.github.io/portfolio/water-cathedral-beauty/","section":"portfolio","summary":"","tags":null,"title":"Houdini Flip"},{"categories":null,"date":"January 1, 0001","permalink":"https://kampter.github.io/portfolio/lighting-impact/","section":"portfolio","summary":"","tags":null,"title":"Lighting Impact"},{"categories":null,"date":"January 1, 0001","permalink":"https://kampter.github.io/portfolio/particle-trails/","section":"portfolio","summary":"","tags":null,"title":"Particle Trails"},{"categories":null,"date":"January 1, 0001","permalink":"https://kampter.github.io/portfolio/poison-projectiles/","section":"portfolio","summary":"实时\n","tags":null,"title":"Poison Projectiles"},{"categories":null,"date":"January 1, 0001","permalink":"https://kampter.github.io/portfolio/bubble-effect/","section":"portfolio","summary":"","tags":null,"title":"Portal Effect"},{"categories":null,"date":"January 1, 0001","permalink":"https://kampter.github.io/portfolio/portal-effect/","section":"portfolio","summary":"","tags":null,"title":"Portal Effect"},{"categories":null,"date":"January 1, 0001","permalink":"https://kampter.github.io/portfolio/glitch/","section":"portfolio","summary":"","tags":null,"title":"Post Processing Glitch"},{"categories":null,"date":"January 1, 0001","permalink":"https://kampter.github.io/portfolio/%E8%8D%89%E5%9C%B0%E6%B8%B2%E6%9F%93/","section":"portfolio","summary":"","tags":null,"title":"Procedural Grass Render"},{"categories":null,"date":"January 1, 0001","permalink":"https://kampter.github.io/portfolio/%E5%B1%8F%E5%B9%95%E7%A9%BA%E9%97%B4%E9%9B%BE%E6%95%88/","section":"portfolio","summary":"","tags":null,"title":"Screen Space Fog"},{"categories":null,"date":"January 1, 0001","permalink":"https://kampter.github.io/portfolio/%E5%B1%8F%E5%B9%95%E7%A9%BA%E9%97%B4%E6%8F%8F%E8%BE%B9/","section":"portfolio","summary":"","tags":null,"title":"Screen Space Outline"},{"categories":null,"date":"January 1, 0001","permalink":"https://kampter.github.io/portfolio/%E5%86%9B%E5%9B%A2%E8%A6%81%E5%A1%9E2%E6%B8%B2%E6%9F%93/","section":"portfolio","summary":"","tags":null,"title":"Team Fortress 2 Rendering"}]